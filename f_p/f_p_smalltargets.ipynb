{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd59b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[targets] drugs total=379, with>=1 target=264\n",
      "[HVG] token_ids: 4000\n",
      "[subset] HVG ∪ TARGETS size: 4184\n",
      "[target-only] size: 278\n",
      "[vocab] VOCAB_SIZE: 4188 | N_SPECIAL: 4\n",
      "[targets] drugs with>=1 target vec: 264\n",
      "[organ] NUM_ORGANS: 16 | mapped cell_lines: 102 | UNK_ORGAN_ID: 0\n",
      "[SMILES] missing=0/379 | zero_vec=2\n",
      "[SMILES] bank: (379, 768)\n",
      "[baseline] global: (62713,) | by_cl: 50\n",
      "[split] train pairs: 10505 | val pairs: 1168\n",
      "[parquet] files: 3388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Index parquet row-groups: 100%|██████████| 3388/3388 [13:31<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[parquet] indexed pairs: 11673\n",
      "✅ token_emb loaded: 4184/4184\n",
      "[target_sub_ids]: (278,)\n",
      ">>> TRAIN START: FP(TARGET-ONLY) + ORGAN + SMILES CLIP | Variant A(log1p->delta->clip) | NO pos_emb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 7000/7000 [2:47:20<00:00,  1.43s/it, lr=1.00e-04, tau=0.106, tot=6.4425, tgt=6.1915, clip=4.8591, align=0.3241, rank(last)=5.1235, Hit@5=0.018, TrueCos=0.676, upd=1750/1750]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 1/20] lr=1.00e-04 | tau=0.106 | train_total=6.4425 | train_tgt=6.1915 | train_clip=4.8591 | train_align=0.3241 | rank_last=5.1235 | Hit@5=0.018 | TrueCos=0.676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/site-packages/torch/nn/modules/transformer.py:515: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ VALID: {'Recall@5': 0.13453268006588323, 'Precision@5': 0.03774479166666671, 'Recall@10': 0.21400091495599294, 'Precision@10': 0.03110416666666666, 'SMILES_Hit@1': 0.011770833333333333, 'SMILES_Hit@5': 0.03763020833333333, 'SMILES_Hit@10': 0.055546875, 'SMILES_TrueCos': 0.761170000632604, 'SMILES_CLIP': 4.818466658592224, 'tau': 0.10604571551084518}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 7000/7000 [3:02:26<00:00,  1.56s/it, lr=9.93e-05, tau=0.091, tot=6.0259, tgt=5.7825, clip=4.7293, align=0.2790, rank(last)=4.9658, Hit@5=0.062, TrueCos=0.721, upd=1750/1750]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 2/20] lr=9.93e-05 | tau=0.091 | train_total=6.0259 | train_tgt=5.7825 | train_clip=4.7293 | train_align=0.2790 | rank_last=4.9658 | Hit@5=0.062 | TrueCos=0.721\n",
      "✅ VALID: {'Recall@5': 0.1812032657013125, 'Precision@5': 0.05100000000000001, 'Recall@10': 0.2784314728963165, 'Precision@10': 0.04054687500000003, 'SMILES_Hit@1': 0.029817708333333335, 'SMILES_Hit@5': 0.07591145833333333, 'SMILES_Hit@10': 0.11114583333333333, 'SMILES_TrueCos': 0.6834968763589859, 'SMILES_CLIP': 4.665763538678487, 'tau': 0.0910659059882164}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 7000/7000 [3:06:25<00:00,  1.60s/it, lr=9.73e-05, tau=0.077, tot=5.7821, tgt=5.5438, clip=4.5923, align=0.3492, rank(last)=4.8268, Hit@5=0.104, TrueCos=0.651, upd=1750/1750]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 3/20] lr=9.73e-05 | tau=0.077 | train_total=5.7821 | train_tgt=5.5438 | train_clip=4.5923 | train_align=0.3492 | rank_last=4.8268 | Hit@5=0.104 | TrueCos=0.651\n",
      "✅ VALID: {'Recall@5': 0.2047391628192409, 'Precision@5': 0.05968229166666662, 'Recall@10': 0.31234690584299996, 'Precision@10': 0.046921874999999995, 'SMILES_Hit@1': 0.051744791666666665, 'SMILES_Hit@5': 0.115703125, 'SMILES_Hit@10': 0.15895833333333334, 'SMILES_TrueCos': 0.620230129758517, 'SMILES_CLIP': 4.5414337539672855, 'tau': 0.07650987803936005}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 7000/7000 [2:58:05<00:00,  1.53s/it, lr=9.40e-05, tau=0.066, tot=5.6183, tgt=5.3850, clip=4.4654, align=0.4042, rank(last)=4.6868, Hit@5=0.138, TrueCos=0.596, upd=1750/1750]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 4/20] lr=9.40e-05 | tau=0.066 | train_total=5.6183 | train_tgt=5.3850 | train_clip=4.4654 | train_align=0.4042 | rank_last=4.6868 | Hit@5=0.138 | TrueCos=0.596\n",
      "✅ VALID: {'Recall@5': 0.22050381562881563, 'Precision@5': 0.06540104166666663, 'Recall@10': 0.3277380745701059, 'Precision@10': 0.050192708333333357, 'SMILES_Hit@1': 0.06661458333333334, 'SMILES_Hit@5': 0.14067708333333334, 'SMILES_Hit@10': 0.18861979166666668, 'SMILES_TrueCos': 0.5752571612596512, 'SMILES_CLIP': 4.431879811286926, 'tau': 0.06598414480686188}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 7000/7000 [3:07:13<00:00,  1.60s/it, lr=8.95e-05, tau=0.057, tot=5.4982, tgt=5.2693, clip=4.3565, align=0.4428, rank(last)=4.6511, Hit@5=0.164, TrueCos=0.557, upd=1750/1750]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 5/20] lr=8.95e-05 | tau=0.057 | train_total=5.4982 | train_tgt=5.2693 | train_clip=4.3565 | train_align=0.4428 | rank_last=4.6511 | Hit@5=0.164 | TrueCos=0.557\n",
      "✅ VALID: {'Recall@5': 0.23745490244709003, 'Precision@5': 0.07045312500000005, 'Recall@10': 0.3467131688479345, 'Precision@10': 0.05305468750000002, 'SMILES_Hit@1': 0.07565104166666667, 'SMILES_Hit@5': 0.16158854166666667, 'SMILES_Hit@10': 0.21513020833333332, 'SMILES_TrueCos': 0.5383901741107305, 'SMILES_CLIP': 4.329053503672282, 'tau': 0.05719948932528496}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 7000/7000 [2:23:35<00:00,  1.23s/it, lr=8.39e-05, tau=0.050, tot=5.3934, tgt=5.1692, clip=4.2465, align=0.4753, rank(last)=4.5532, Hit@5=0.189, TrueCos=0.525, upd=1750/1750]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 6/20] lr=8.39e-05 | tau=0.050 | train_total=5.3934 | train_tgt=5.1692 | train_clip=4.2465 | train_align=0.4753 | rank_last=4.5532 | Hit@5=0.189 | TrueCos=0.525\n",
      "✅ VALID: {'Recall@5': 0.24949122723341488, 'Precision@5': 0.07468229166666664, 'Recall@10': 0.36050634030321543, 'Precision@10': 0.055804687499999964, 'SMILES_Hit@1': 0.08911458333333333, 'SMILES_Hit@5': 0.19122395833333333, 'SMILES_Hit@10': 0.250546875, 'SMILES_TrueCos': 0.5103278501828512, 'SMILES_CLIP': 4.231426575183868, 'tau': 0.05004861205816269}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 7000/7000 [2:14:16<00:00,  1.15s/it, lr=7.74e-05, tau=0.044, tot=5.3107, tgt=5.0908, clip=4.1492, align=0.4995, rank(last)=4.5271, Hit@5=0.210, TrueCos=0.501, upd=1750/1750]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 7/20] lr=7.74e-05 | tau=0.044 | train_total=5.3107 | train_tgt=5.0908 | train_clip=4.1492 | train_align=0.4995 | rank_last=4.5271 | Hit@5=0.210 | TrueCos=0.501\n",
      "✅ VALID: {'Recall@5': 0.25910566350605413, 'Precision@5': 0.07683333333333335, 'Recall@10': 0.3705589896214895, 'Precision@10': 0.056846354166666654, 'SMILES_Hit@1': 0.09348958333333333, 'SMILES_Hit@5': 0.19731770833333334, 'SMILES_Hit@10': 0.2579427083333333, 'SMILES_TrueCos': 0.49005644301573437, 'SMILES_CLIP': 4.156260814666748, 'tau': 0.04421854019165039}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 7000/7000 [3:03:08<00:00,  1.57s/it, lr=7.01e-05, tau=0.040, tot=5.2454, tgt=5.0293, clip=4.0621, align=0.5202, rank(last)=4.4614, Hit@5=0.227, TrueCos=0.480, upd=1750/1750]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 8/20] lr=7.01e-05 | tau=0.040 | train_total=5.2454 | train_tgt=5.0293 | train_clip=4.0621 | train_align=0.5202 | rank_last=4.4614 | Hit@5=0.227 | TrueCos=0.480\n",
      "✅ VALID: {'Recall@5': 0.2628372523020961, 'Precision@5': 0.07845312499999996, 'Recall@10': 0.37603251551689054, 'Precision@10': 0.058479166666666686, 'SMILES_Hit@1': 0.10135416666666666, 'SMILES_Hit@5': 0.21856770833333333, 'SMILES_Hit@10': 0.28325520833333334, 'SMILES_TrueCos': 0.47121061543623605, 'SMILES_CLIP': 4.096646081606547, 'tau': 0.03951994702219963}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 7000/7000 [3:37:14<00:00,  1.86s/it, lr=6.23e-05, tau=0.036, tot=5.1898, tgt=4.9772, clip=3.9836, align=0.5375, rank(last)=4.5038, Hit@5=0.242, TrueCos=0.463, upd=1750/1750]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 9/20] lr=6.23e-05 | tau=0.036 | train_total=5.1898 | train_tgt=4.9772 | train_clip=3.9836 | train_align=0.5375 | rank_last=4.5038 | Hit@5=0.242 | TrueCos=0.463\n",
      "✅ VALID: {'Recall@5': 0.2748946131270349, 'Precision@5': 0.0824583333333334, 'Recall@10': 0.3857195520769744, 'Precision@10': 0.059971354166666636, 'SMILES_Hit@1': 0.11109375, 'SMILES_Hit@5': 0.23278645833333333, 'SMILES_Hit@10': 0.29932291666666666, 'SMILES_TrueCos': 0.4531615019838015, 'SMILES_CLIP': 4.0104965694745385, 'tau': 0.03574628755450249}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 7000/7000 [3:58:35<00:00,  2.05s/it, lr=5.42e-05, tau=0.033, tot=5.1428, tgt=4.9334, clip=3.9123, align=0.5507, rank(last)=4.4189, Hit@5=0.256, TrueCos=0.449, upd=1750/1750]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 10/20] lr=5.42e-05 | tau=0.033 | train_total=5.1428 | train_tgt=4.9334 | train_clip=3.9123 | train_align=0.5507 | rank_last=4.4189 | Hit@5=0.256 | TrueCos=0.449\n",
      "✅ VALID: {'Recall@5': 0.27848910081527267, 'Precision@5': 0.08344791666666675, 'Recall@10': 0.3921030132338726, 'Precision@10': 0.061320312499999995, 'SMILES_Hit@1': 0.11471354166666667, 'SMILES_Hit@5': 0.243046875, 'SMILES_Hit@10': 0.311875, 'SMILES_TrueCos': 0.4423800575733185, 'SMILES_CLIP': 3.955232696533203, 'tau': 0.032634906470775604}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 7000/7000 [3:29:00<00:00,  1.79s/it, lr=4.59e-05, tau=0.030, tot=5.1065, tgt=4.8998, clip=3.8539, align=0.5616, rank(last)=4.4987, Hit@5=0.266, TrueCos=0.438, upd=1750/1750]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 11/20] lr=4.59e-05 | tau=0.030 | train_total=5.1065 | train_tgt=4.8998 | train_clip=3.8539 | train_align=0.5616 | rank_last=4.4987 | Hit@5=0.266 | TrueCos=0.438\n",
      "✅ VALID: {'Recall@5': 0.2823159555288459, 'Precision@5': 0.08485937500000001, 'Recall@10': 0.3932350697624135, 'Precision@10': 0.06134374999999996, 'SMILES_Hit@1': 0.11958333333333333, 'SMILES_Hit@5': 0.24731770833333333, 'SMILES_Hit@10': 0.316015625, 'SMILES_TrueCos': 0.4323416962226232, 'SMILES_CLIP': 3.9188380829493203, 'tau': 0.03016114979982376}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 7000/7000 [3:23:54<00:00,  1.75s/it, lr=3.78e-05, tau=0.028, tot=5.0765, tgt=4.8721, clip=3.8035, align=0.5707, rank(last)=4.3034, Hit@5=0.274, TrueCos=0.429, upd=1750/1750]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 12/20] lr=3.78e-05 | tau=0.028 | train_total=5.0765 | train_tgt=4.8721 | train_clip=3.8035 | train_align=0.5707 | rank_last=4.3034 | Hit@5=0.274 | TrueCos=0.429\n",
      "✅ VALID: {'Recall@5': 0.28495215519434286, 'Precision@5': 0.08613541666666664, 'Recall@10': 0.4008907776251527, 'Precision@10': 0.06268229166666665, 'SMILES_Hit@1': 0.12125, 'SMILES_Hit@5': 0.2575, 'SMILES_Hit@10': 0.325234375, 'SMILES_TrueCos': 0.42219115207592645, 'SMILES_CLIP': 3.8668323413530987, 'tau': 0.02826393023133278}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 7000/7000 [2:16:37<00:00,  1.17s/it, lr=3.00e-05, tau=0.027, tot=5.0510, tgt=4.8485, clip=3.7606, align=0.5776, rank(last)=4.3273, Hit@5=0.281, TrueCos=0.422, upd=1750/1750]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 13/20] lr=3.00e-05 | tau=0.027 | train_total=5.0510 | train_tgt=4.8485 | train_clip=3.7606 | train_align=0.5776 | rank_last=4.3273 | Hit@5=0.281 | TrueCos=0.422\n",
      "✅ VALID: {'Recall@5': 0.28949838233236663, 'Precision@5': 0.08839062500000006, 'Recall@10': 0.40374855960012207, 'Precision@10': 0.0638854166666667, 'SMILES_Hit@1': 0.12619791666666666, 'SMILES_Hit@5': 0.2640104166666667, 'SMILES_Hit@10': 0.33171875, 'SMILES_TrueCos': 0.4184157766898473, 'SMILES_CLIP': 3.8417517272631327, 'tau': 0.026800617575645447}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 7000/7000 [2:07:05<00:00,  1.09s/it, lr=2.27e-05, tau=0.026, tot=5.0314, tgt=4.8305, clip=3.7266, align=0.5827, rank(last)=4.2995, Hit@5=0.287, TrueCos=0.417, upd=1750/1750]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 14/20] lr=2.27e-05 | tau=0.026 | train_total=5.0314 | train_tgt=4.8305 | train_clip=3.7266 | train_align=0.5827 | rank_last=4.2995 | Hit@5=0.287 | TrueCos=0.417\n",
      "✅ VALID: {'Recall@5': 0.2882152030550469, 'Precision@5': 0.0881875, 'Recall@10': 0.4061875874414938, 'Precision@10': 0.06410677083333335, 'SMILES_Hit@1': 0.12630208333333334, 'SMILES_Hit@5': 0.26114583333333335, 'SMILES_Hit@10': 0.33466145833333333, 'SMILES_TrueCos': 0.41290168126424154, 'SMILES_CLIP': 3.8126182357470193, 'tau': 0.025704631581902504}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 7000/7000 [2:20:48<00:00,  1.21s/it, lr=1.62e-05, tau=0.025, tot=5.0146, tgt=4.8151, clip=3.6979, align=0.5870, rank(last)=4.3114, Hit@5=0.291, TrueCos=0.413, upd=1750/1750]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 15/20] lr=1.62e-05 | tau=0.025 | train_total=5.0146 | train_tgt=4.8151 | train_clip=3.6979 | train_align=0.5870 | rank_last=4.3114 | Hit@5=0.291 | TrueCos=0.413\n",
      "✅ VALID: {'Recall@5': 0.2933345646685491, 'Precision@5': 0.08872916666666665, 'Recall@10': 0.40821422371031785, 'Precision@10': 0.06409375000000002, 'SMILES_Hit@1': 0.127265625, 'SMILES_Hit@5': 0.267734375, 'SMILES_Hit@10': 0.340703125, 'SMILES_TrueCos': 0.4091666708389918, 'SMILES_CLIP': 3.799344880580902, 'tau': 0.024924729019403458}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 7000/7000 [2:27:37<00:00,  1.27s/it, lr=1.06e-05, tau=0.024, tot=5.0031, tgt=4.8044, clip=3.6800, align=0.5901, rank(last)=4.2728, Hit@5=0.295, TrueCos=0.410, upd=1750/1750]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 16/20] lr=1.06e-05 | tau=0.024 | train_total=5.0031 | train_tgt=4.8044 | train_clip=3.6800 | train_align=0.5901 | rank_last=4.2728 | Hit@5=0.295 | TrueCos=0.410\n",
      "✅ VALID: {'Recall@5': 0.29402793040293046, 'Precision@5': 0.0888958333333333, 'Recall@10': 0.41049326382529516, 'Precision@10': 0.06436718749999999, 'SMILES_Hit@1': 0.12734375, 'SMILES_Hit@5': 0.2702864583333333, 'SMILES_Hit@10': 0.34197916666666667, 'SMILES_TrueCos': 0.406905122200648, 'SMILES_CLIP': 3.781051870981852, 'tau': 0.02439727634191513}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  29%|██▉       | 2032/7000 [1:01:00<2:29:09,  1.80s/it, lr=9.22e-06, tau=0.024, tot=4.9950, tgt=4.7968, clip=3.6670, align=0.5911, rank(last)=4.3602, Hit@5=0.297, TrueCos=0.409, upd=500/1750]\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs00000000865c7a3a000a1736'\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs00000000865c7a35000a1735'\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000009655b40e000a1734'\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs00000000865c7a34000a1733'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1506\u001b[0m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>>> TRAIN START: FP(TARGET-ONLY) + ORGAN + SMILES CLIP | Variant A(log1p->delta->clip) | NO pos_emb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m-> 1506\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch_fixed_steps\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1510\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSTEPS_PER_EPOCH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1513\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_sub_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_sub_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1516\u001b[0m \u001b[43m        \u001b[49m\u001b[43msmiles_bank_t\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msmiles_bank_t\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_clip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_GRAD_NORM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1519\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mACCUM_STEPS\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1520\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m   1523\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1524\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | tau=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtau\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1530\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHit@5=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_hit5\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | TrueCos=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_truecos\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1531\u001b[0m     )\n\u001b[1;32m   1533\u001b[0m     valid \u001b[38;5;241m=\u001b[39m evaluate_fp(\n\u001b[1;32m   1534\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   1535\u001b[0m         loader\u001b[38;5;241m=\u001b[39mval_loader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1540\u001b[0m         hitk\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m10\u001b[39m),\n\u001b[1;32m   1541\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[1], line 1354\u001b[0m, in \u001b[0;36mtrain_one_epoch_fixed_steps\u001b[0;34m(model, train_loader, device, steps_per_epoch, optimizer, scheduler, scaler, target_sub_ids, pos_weight, smiles_bank_t, log_every, grad_clip, accum_steps)\u001b[0m\n\u001b[1;32m   1351\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, steps_per_epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, dynamic_ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[0;32m-> 1354\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1356\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1357\u001b[0m     values    \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[1], line 1297\u001b[0m, in \u001b[0;36minfinite_loader\u001b[0;34m(loader)\u001b[0m\n\u001b[1;32m   1295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minfinite_loader\u001b[39m(loader):\n\u001b[1;32m   1296\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1297\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m   1298\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m b\n",
      "File \u001b[0;32m/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/site-packages/torch/utils/data/dataloader.py:732\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 732\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    738\u001b[0m ):\n",
      "File \u001b[0;32m/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1482\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data, worker_id)\n\u001b[1;32m   1481\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1482\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1483\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1485\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1434\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1432\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1433\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1434\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1435\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1436\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1275\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1263\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1272\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1273\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1274\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1275\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1276\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1277\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1278\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# FP (TARGET-ONLY) + ORGAN TOKEN + SMILES CLIP  \n",
    "# Variant A scaling: raw counts -> log1p -> delta -> clip (+ optional asinh)\n",
    "# Drop first \"service\" element from genes/expr\n",
    "# Stable gene order after selection: sort by gene_id (deterministic)\n",
    "# ORGAN token injected at fixed position [CLS][ORGAN] (NO cell-line embedding)\n",
    "# Positional embeddings REMOVED\n",
    "# Targets: cosine vector loss + cosine-BCE (neg sampling) + InfoNCE rank\n",
    "# SMILES: CLIP loss (+ optional cosine align), NO SupCon, NO MSE\n",
    "# Batch is CLIP-safe: unique drug in batch (1 cell per drug)\n",
    "# Skip missing/zero SMILES vectors\n",
    "# OneCycleLR fixed for grad accumulation (total_steps = real updates)\n",
    "# AMP + grad accumulation\n",
    "# =========================================================\n",
    "\n",
    "import os, glob, ast, random\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import scanpy as sc\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 0) PATHS / HYPERPARAMS\n",
    "# =========================================================\n",
    "PARQUET_DIR    = \"/data/aiffel/data/Tahoe-100M/data\"\n",
    "GENE_META_PATH = \"/data/aiffel/data/Tahoe-100M/metadata/gene_metadata.parquet\"\n",
    "DRUG_META_PATH = \"/data/aiffel/data/Tahoe-100M/metadata/drug_metadata.parquet\"\n",
    "COUNTS_CSV     = \"/data/aiffel/babayakga/making_data/aiffel/babayakga/making_data/tahoe_counts_per_drug_cell_line.csv\"\n",
    "DMSO_PATH      = \"/data/aiffel/babayakga/outputs/dmso.h5ad\"\n",
    "CELL_LINE_META_PATH = \"/data/aiffel/data/Tahoe-100M/metadata/cell_line_metadata.parquet\"\n",
    "\n",
    "SMILES_EMB_PATH       = \"/data/aiffel/babayakga/smiles_emb/drug_smiles_emb_all1.pt\"\n",
    "PRETRAINED_GENE_NPY   = \"/data/aiffel/babayakga/pretraining/checkpoints_with_cell/gene_embeddings.npy\"  # optional\n",
    "\n",
    "CONTROL_DRUG = \"DMSO_TF\"\n",
    "SEED = 42\n",
    "\n",
    "# sequence\n",
    "MAX_SEQ_LEN = 256\n",
    "HVG_K = 4000\n",
    "\n",
    "# training\n",
    "BATCH_SIZE  = 128\n",
    "ACCUM_STEPS = 4\n",
    "STEPS_PER_EPOCH = 7000\n",
    "VAL_STEPS       = 300\n",
    "EPOCHS          = 20\n",
    "\n",
    "LR           = 1e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "MAX_GRAD_NORM = 1.0\n",
    "\n",
    "# targets loss weights \n",
    "lambda_cos  = 1.0\n",
    "lambda_bce  = 0.05\n",
    "lambda_rank = 1.0\n",
    "\n",
    "# bce/rank knobs\n",
    "bce_num_neg  = 2048\n",
    "bce_pos_cap  = None\n",
    "tau_bce      = 0.15\n",
    "\n",
    "rank_num_neg = 1024\n",
    "rank_num_pos = 4\n",
    "tau_rank     = 0.15\n",
    "\n",
    "# SMILES (CLIP)\n",
    "lambda_smiles = 0.05\n",
    "alpha_align   = 0.5\n",
    "TAU_INIT      = 0.10  # initial tau (learnable temperature)\n",
    "\n",
    "# overall mixing\n",
    "lambda_targets = 1.0  \n",
    "\n",
    "# data sampling\n",
    "NUM_WORKERS = 4\n",
    "MIN_TRAIN_CELLS_PER_PAIR = 1000\n",
    "TEST_SIZE = 0.1\n",
    "\n",
    "# Variant A scaling\n",
    "USE_LOG1P_EXPR   = True\n",
    "USE_ASINH_DELTA  = False\n",
    "DELTA_CLIP_ABS   = 5.0\n",
    "\n",
    "# misc\n",
    "DROP_FIRST_GENE_TOKEN = True\n",
    "\n",
    "# device\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    try:\n",
    "        torch.set_float32_matmul_precision(\"high\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 1) gene_metadata\n",
    "# =========================================================\n",
    "gene_md = pd.read_parquet(GENE_META_PATH).copy()\n",
    "gene_md[\"gene_symbol\"] = gene_md[\"gene_symbol\"].astype(str)\n",
    "gene_md[\"ensembl_id\"]  = gene_md[\"ensembl_id\"].astype(str)\n",
    "gene_md[\"token_id\"]    = gene_md[\"token_id\"].astype(int)\n",
    "gene_md = gene_md.sort_values(\"token_id\").reset_index(drop=True)\n",
    "\n",
    "N_GENES = int(gene_md[\"token_id\"].max()) + 1\n",
    "symbol_to_ensg_lower = dict(zip(gene_md[\"gene_symbol\"].str.lower(), gene_md[\"ensembl_id\"]))\n",
    "ensg_to_token_id = dict(zip(gene_md[\"ensembl_id\"].values, gene_md[\"token_id\"].values))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2) drug_metadata -> targets\n",
    "# =========================================================\n",
    "def parse_targets(x):\n",
    "    if x is None:\n",
    "        return []\n",
    "    if isinstance(x, float) and np.isnan(x):\n",
    "        return []\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return [str(t).strip() for t in x if str(t).strip()]\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip()\n",
    "        if (s.startswith(\"[\") and s.endswith(\"]\")) or (s.startswith(\"(\") and s.endswith(\")\")):\n",
    "            try:\n",
    "                out = ast.literal_eval(s)\n",
    "                if isinstance(out, (list, tuple)):\n",
    "                    return [str(t).strip() for t in out if str(t).strip()]\n",
    "            except Exception:\n",
    "                pass\n",
    "        for sep in [\";\", \",\"]:\n",
    "            if sep in s:\n",
    "                return [t.strip() for t in s.split(sep) if t.strip()]\n",
    "        return [s] if s else []\n",
    "    return [str(x).strip()]\n",
    "\n",
    "drug_meta_df = pd.read_parquet(DRUG_META_PATH).copy()\n",
    "drug_meta_df[\"drug\"] = drug_meta_df[\"drug\"].astype(str)\n",
    "\n",
    "drug_to_target_tokenids = {}\n",
    "all_target_tokenids = set()\n",
    "\n",
    "for _, row in drug_meta_df.iterrows():\n",
    "    drug = str(row[\"drug\"])\n",
    "    targets = parse_targets(row.get(\"targets\", None))\n",
    "\n",
    "    tids = []\n",
    "    for t in targets:\n",
    "        t = str(t).strip()\n",
    "        if not t:\n",
    "            continue\n",
    "        if t.startswith(\"ENSG\"):\n",
    "            ensg = t\n",
    "        else:\n",
    "            ensg = symbol_to_ensg_lower.get(t.lower(), None)\n",
    "        if ensg is None:\n",
    "            continue\n",
    "        tid = ensg_to_token_id.get(ensg, None)\n",
    "        if tid is None:\n",
    "            continue\n",
    "        tid = int(tid)\n",
    "        if 0 <= tid < N_GENES:\n",
    "            tids.append(tid)\n",
    "\n",
    "    tids = sorted(set(tids))\n",
    "    drug_to_target_tokenids[drug] = tids\n",
    "    all_target_tokenids.update(tids)\n",
    "\n",
    "drug_has_targets = {d: (len(tids) > 0) for d, tids in drug_to_target_tokenids.items()}\n",
    "print(f\"[targets] drugs total={len(drug_to_target_tokenids)}, with>=1 target={sum(drug_has_targets.values())}\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 3) HVG from DMSO\n",
    "# =========================================================\n",
    "def compute_hvg_token_ids_from_dmso(dmso_h5ad_path: str, control_drug: str, HVG_K: int, ensg_to_token_id: dict):\n",
    "    ad = sc.read_h5ad(dmso_h5ad_path)\n",
    "    obs = ad.obs\n",
    "    m = (obs[\"drug\"].astype(str).values == str(control_drug))\n",
    "    idx = np.where(m)[0]\n",
    "    if idx.size == 0:\n",
    "        raise ValueError(f\"No DMSO cells found: control_drug={control_drug}\")\n",
    "\n",
    "    X = ad.X.tocsr() if sparse.issparse(ad.X) else sparse.csr_matrix(ad.X)\n",
    "    Xc = X[idx]\n",
    "\n",
    "    mean = np.asarray(Xc.mean(axis=0)).ravel()\n",
    "    mean2 = np.asarray(Xc.multiply(Xc).mean(axis=0)).ravel()\n",
    "    var = (mean2 - mean**2).astype(np.float32)\n",
    "\n",
    "    ensgs = ad.var_names.astype(str).tolist()\n",
    "    token_ids, vars_ = [], []\n",
    "    for j, ensg in enumerate(ensgs):\n",
    "        tid = ensg_to_token_id.get(ensg, None)\n",
    "        if tid is None:\n",
    "            continue\n",
    "        token_ids.append(int(tid))\n",
    "        vars_.append(float(var[j]))\n",
    "\n",
    "    token_ids = np.asarray(token_ids, dtype=np.int64)\n",
    "    vars_ = np.asarray(vars_, dtype=np.float32)\n",
    "    if token_ids.size == 0:\n",
    "        raise ValueError(\"ENSG mapping failed.\")\n",
    "\n",
    "    k = min(int(HVG_K), token_ids.size)\n",
    "    top = np.argpartition(-vars_, k-1)[:k]\n",
    "    return set(token_ids[top].tolist())\n",
    "\n",
    "hvg_token_ids = compute_hvg_token_ids_from_dmso(DMSO_PATH, CONTROL_DRUG, HVG_K, ensg_to_token_id)\n",
    "print(\"[HVG] token_ids:\", len(hvg_token_ids))\n",
    "\n",
    "# INPUT subset = HVG ∪ TARGETS\n",
    "subset_token_ids = sorted(set(hvg_token_ids) | set(all_target_tokenids))\n",
    "M_SUB = len(subset_token_ids)\n",
    "print(\"[subset] HVG ∪ TARGETS size:\", M_SUB)\n",
    "\n",
    "# OUTPUT target-only = TARGETS only\n",
    "target_token_ids = sorted(set(all_target_tokenids))\n",
    "M_TGT = len(target_token_ids)\n",
    "print(\"[target-only] size:\", M_TGT)\n",
    "\n",
    "old_tid_to_subid = {tid: i for i, tid in enumerate(subset_token_ids)}\n",
    "old_tid_to_tgtid = {tid: i for i, tid in enumerate(target_token_ids)}\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 4) vocab + LUT (include ORGAN token)\n",
    "# =========================================================\n",
    "SPECIAL_TOKENS = [\"[PAD]\", \"[CLS]\", \"[ORGAN]\", \"[MASK]\"]\n",
    "local_token_to_id = {tok: i for i, tok in enumerate(SPECIAL_TOKENS)}\n",
    "N_SPECIAL = len(SPECIAL_TOKENS)\n",
    "\n",
    "VOCAB_SIZE = N_SPECIAL + M_SUB\n",
    "PAD_ID   = local_token_to_id[\"[PAD]\"]\n",
    "CLS_ID   = local_token_to_id[\"[CLS]\"]\n",
    "ORGAN_TOK_ID = local_token_to_id[\"[ORGAN]\"]\n",
    "\n",
    "old_tid_to_vocab_lut = np.full((N_GENES,), -1, dtype=np.int64)\n",
    "for sid, old_tid in enumerate(subset_token_ids):\n",
    "    if 0 <= old_tid < N_GENES:\n",
    "        old_tid_to_vocab_lut[old_tid] = N_SPECIAL + sid\n",
    "\n",
    "subset_token_ids_np = np.asarray(subset_token_ids, dtype=np.int64)\n",
    "print(\"[vocab] VOCAB_SIZE:\", VOCAB_SIZE, \"| N_SPECIAL:\", N_SPECIAL)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 5) y_targets (drug -> TARGET-ONLY multi-hot)\n",
    "# =========================================================\n",
    "drug_to_target_vec_tgt = {}\n",
    "for d, tids in drug_to_target_tokenids.items():\n",
    "    vec = np.zeros(M_TGT, dtype=np.float32)\n",
    "    for tid in tids:\n",
    "        j = old_tid_to_tgtid.get(int(tid), None)\n",
    "        if j is not None:\n",
    "            vec[j] = 1.0\n",
    "    drug_to_target_vec_tgt[d] = vec\n",
    "\n",
    "print(\"[targets] drugs with>=1 target vec:\", sum(float(v.sum()) > 0 for v in drug_to_target_vec_tgt.values()))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 6) organ mapping: cell_line_id -> organ_id   (FIX: UNK=0, organs start at 1)\n",
    "# =========================================================\n",
    "cl_meta = pd.read_parquet(CELL_LINE_META_PATH).copy()\n",
    "cl_meta[\"Cell_ID_Cellosaur\"] = cl_meta[\"Cell_ID_Cellosaur\"].astype(str)\n",
    "cl_meta[\"Organ\"] = cl_meta[\"Organ\"].astype(str)\n",
    "\n",
    "cl_meta_small = cl_meta[[\"Cell_ID_Cellosaur\", \"Organ\"]].dropna().drop_duplicates()\n",
    "organs = sorted(cl_meta_small[\"Organ\"].unique().tolist())\n",
    "\n",
    "UNK_ORGAN_ID = 0\n",
    "organ2id = {o: i+1 for i, o in enumerate(organs)}  # shift by 1\n",
    "NUM_ORGANS = len(organs) + 1\n",
    "\n",
    "cellline2organid = {\n",
    "    str(cvcl): int(organ2id.get(str(org), UNK_ORGAN_ID))\n",
    "    for cvcl, org in cl_meta_small.values\n",
    "}\n",
    "print(\"[organ] NUM_ORGANS:\", NUM_ORGANS, \"| mapped cell_lines:\", len(cellline2organid), \"| UNK_ORGAN_ID:\", UNK_ORGAN_ID)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 7) SMILES embeddings + bank (for retrieval)\n",
    "# =========================================================\n",
    "obj = torch.load(SMILES_EMB_PATH, map_location=\"cpu\")\n",
    "assert isinstance(obj, dict) and \"drug\" in obj and \"emb\" in obj\n",
    "\n",
    "drug_list_saved = [str(d) for d in obj[\"drug\"]]\n",
    "emb_matrix = obj[\"emb\"].to(dtype=torch.float32).cpu().numpy()\n",
    "SMILES_DIM = int(emb_matrix.shape[1])\n",
    "drug_to_smiles_np_raw = {d: emb_matrix[i].astype(np.float32, copy=False) for i, d in enumerate(drug_list_saved)}\n",
    "\n",
    "drug_names_all = sorted(set(drug_meta_df[\"drug\"].astype(str).tolist()))\n",
    "drug2id = {d: i for i, d in enumerate(drug_names_all)}\n",
    "\n",
    "drug_to_smiles_np = {}\n",
    "missing = 0\n",
    "zeroed  = 0\n",
    "for d in drug_names_all:\n",
    "    v = drug_to_smiles_np_raw.get(d, None)\n",
    "    if v is None:\n",
    "        drug_to_smiles_np[d] = np.zeros((SMILES_DIM,), dtype=np.float32)\n",
    "        missing += 1\n",
    "    else:\n",
    "        vv = v.astype(np.float32, copy=False)\n",
    "        if np.abs(vv).sum() == 0.0:\n",
    "            zeroed += 1\n",
    "        drug_to_smiles_np[d] = vv\n",
    "print(f\"[SMILES] missing={missing}/{len(drug_names_all)} | zero_vec={zeroed}\")\n",
    "\n",
    "smiles_bank_np = np.stack([drug_to_smiles_np[d] for d in drug_names_all], axis=0).astype(np.float32)\n",
    "print(\"[SMILES] bank:\", smiles_bank_np.shape)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 8) DMSO baselines (Variant A: log1p BEFORE mean)\n",
    "# =========================================================\n",
    "def build_dmso_baselines_gene_space(dmso_h5ad_path: str, control_drug: str, N_GENES: int, ensg_to_token_id: dict, use_log1p: bool):\n",
    "    adata = sc.read_h5ad(dmso_h5ad_path)\n",
    "    obs = adata.obs\n",
    "    X = adata.X.tocsr() if sparse.issparse(adata.X) else sparse.csr_matrix(adata.X)\n",
    "\n",
    "    m = (obs[\"drug\"].astype(str).values == str(control_drug))\n",
    "    idx = np.where(m)[0]\n",
    "    if idx.size == 0:\n",
    "        raise ValueError(\"No DMSO cells.\")\n",
    "\n",
    "    ensgs = adata.var_names.astype(str).tolist()\n",
    "    token_ids, cols = [], []\n",
    "    for j, ensg in enumerate(ensgs):\n",
    "        tid = ensg_to_token_id.get(ensg, None)\n",
    "        if tid is None:\n",
    "            continue\n",
    "        token_ids.append(int(tid)); cols.append(j)\n",
    "\n",
    "    token_ids = np.asarray(token_ids, dtype=np.int64)\n",
    "    cols = np.asarray(cols, dtype=np.int64)\n",
    "\n",
    "    Xc = X[idx][:, cols]\n",
    "    if use_log1p:\n",
    "        Xc = Xc.copy()\n",
    "        Xc.data = np.log1p(np.clip(Xc.data, a_min=0.0, a_max=None))\n",
    "\n",
    "    mean_global_sub = np.asarray(Xc.mean(axis=0)).ravel().astype(np.float32)\n",
    "    baseline_global = np.zeros(N_GENES, dtype=np.float32)\n",
    "    baseline_global[token_ids] = mean_global_sub\n",
    "\n",
    "    baseline_by_cl = {}\n",
    "    cl_values = obs[\"cell_line_id\"].astype(str).values\n",
    "    for cl in np.unique(cl_values):\n",
    "        cl_idx = np.where(m & (cl_values == cl))[0]\n",
    "        if cl_idx.size == 0:\n",
    "            continue\n",
    "        Xcl = X[cl_idx][:, cols]\n",
    "        if use_log1p:\n",
    "            Xcl = Xcl.copy()\n",
    "            Xcl.data = np.log1p(np.clip(Xcl.data, a_min=0.0, a_max=None))\n",
    "        mean_cl_sub = np.asarray(Xcl.mean(axis=0)).ravel().astype(np.float32)\n",
    "\n",
    "        v = np.zeros(N_GENES, dtype=np.float32)\n",
    "        v[token_ids] = mean_cl_sub\n",
    "        baseline_by_cl[str(cl)] = v\n",
    "\n",
    "    return baseline_global, baseline_by_cl\n",
    "\n",
    "baseline_global, baseline_by_cl = build_dmso_baselines_gene_space(\n",
    "    DMSO_PATH, CONTROL_DRUG, N_GENES, ensg_to_token_id, use_log1p=USE_LOG1P_EXPR\n",
    ")\n",
    "print(\"[baseline] global:\", baseline_global.shape, \"| by_cl:\", len(baseline_by_cl))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 9) split (drug, cell_line) pairs + weights (filter: has targets)\n",
    "# =========================================================\n",
    "DRUG_COL, CELL_COL, N_COL = \"drug\", \"cell_line_id\", \"n_cells\"\n",
    "\n",
    "counts = pd.read_csv(COUNTS_CSV)\n",
    "counts[DRUG_COL] = counts[DRUG_COL].astype(str)\n",
    "counts[CELL_COL] = counts[CELL_COL].astype(str)\n",
    "counts[N_COL]    = counts[N_COL].astype(int)\n",
    "\n",
    "pairs_df = counts[counts[N_COL] >= MIN_TRAIN_CELLS_PER_PAIR][[DRUG_COL, CELL_COL]].drop_duplicates().copy()\n",
    "pairs_df = pairs_df[pairs_df[DRUG_COL] != str(CONTROL_DRUG)].copy()\n",
    "pairs_df = pairs_df[pairs_df[DRUG_COL].map(lambda d: drug_has_targets.get(str(d), False))].copy()\n",
    "pairs_df = pairs_df[pairs_df[DRUG_COL].isin(set(drug2id.keys()))].copy()\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    pairs_df,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=SEED,\n",
    "    stratify=pairs_df[DRUG_COL],\n",
    ")\n",
    "\n",
    "train_pairs = list(zip(train_df[DRUG_COL], train_df[CELL_COL]))\n",
    "val_pairs   = list(zip(val_df[DRUG_COL],   val_df[CELL_COL]))\n",
    "print(\"[split] train pairs:\", len(train_pairs), \"| val pairs:\", len(val_pairs))\n",
    "\n",
    "def make_pair_weights_from_counts(counts_df, pairs, mode=\"inv_sqrt\", eps=1.0):\n",
    "    pair2n = {(str(d), str(c)): int(n) for d, c, n in counts_df[[DRUG_COL, CELL_COL, N_COL]].values}\n",
    "    w = []\n",
    "    for p in pairs:\n",
    "        n = pair2n.get((str(p[0]), str(p[1])), 0)\n",
    "        if mode == \"inv\":\n",
    "            ww = 1.0 / (n + eps)\n",
    "        elif mode == \"inv_log\":\n",
    "            ww = 1.0 / np.log1p(n + eps)\n",
    "        else:\n",
    "            ww = 1.0 / np.sqrt(n + eps)\n",
    "        w.append(float(ww))\n",
    "    w = np.asarray(w, dtype=np.float64)\n",
    "    w = np.clip(w, 0.0, None)\n",
    "    w = w / (w.sum() + 1e-12)\n",
    "    return w\n",
    "\n",
    "w_train = make_pair_weights_from_counts(counts, train_pairs, mode=\"inv_sqrt\")\n",
    "w_val   = make_pair_weights_from_counts(counts, val_pairs,   mode=\"inv_sqrt\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 10) parquet row-group indexing\n",
    "# =========================================================\n",
    "PARQUET_FILES = sorted(glob.glob(os.path.join(PARQUET_DIR, \"**\", \"*.parquet\"), recursive=True))\n",
    "print(\"[parquet] files:\", len(PARQUET_FILES))\n",
    "\n",
    "def build_pair_to_locations(parquet_files, valid_pairs_set, drug_col=\"drug\", cell_col=\"cell_line_id\"):\n",
    "    out = defaultdict(list)\n",
    "    for f in tqdm(parquet_files, desc=\"Index parquet row-groups\", dynamic_ncols=True):\n",
    "        pf = pq.ParquetFile(f)\n",
    "        for rg in range(pf.num_row_groups):\n",
    "            tbl = pf.read_row_group(rg, columns=[drug_col, cell_col])\n",
    "            df = tbl.to_pandas()\n",
    "            pairs_here = set(zip(df[drug_col].astype(str), df[cell_col].astype(str)))\n",
    "            inter = pairs_here.intersection(valid_pairs_set)\n",
    "            for p in inter:\n",
    "                out[p].append((f, rg))\n",
    "    return out\n",
    "\n",
    "valid_pairs_set = set(train_pairs) | set(val_pairs)\n",
    "pair_to_locations = build_pair_to_locations(PARQUET_FILES, valid_pairs_set, drug_col=DRUG_COL, cell_col=CELL_COL)\n",
    "print(\"[parquet] indexed pairs:\", len(pair_to_locations))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 11) Dataset (unique drug batch + Variant A + stable ordering + organ_id)\n",
    "# =========================================================\n",
    "class TahoeFPParquetDataset_UniqueDrug(torch.utils.data.IterableDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        pair_to_locations,\n",
    "        pairs,\n",
    "        baseline_global,\n",
    "        baseline_by_cellline,\n",
    "        drug_to_target_vec_target_only,   # (M_TGT,)\n",
    "        drug2id,\n",
    "        drug_to_smiles_np,\n",
    "        cellline2organid,\n",
    "        unk_organ_id,\n",
    "        n_genes_full,\n",
    "        steps,\n",
    "        max_seq_len=256,\n",
    "        batch_size=128,\n",
    "        control_drug=\"DMSO_TF\",\n",
    "        pad_id=0,\n",
    "        cls_id=1,\n",
    "        organtok_id=2,\n",
    "        pair_weights=None,\n",
    "        seed=42,\n",
    "        drug_col=\"drug\",\n",
    "        cell_col=\"cell_line_id\",\n",
    "        genes_col=\"genes\",\n",
    "        expr_col=\"expressions\",\n",
    "        cap_per_pair_in_rg=None,\n",
    "        max_tries_per_pair=20,\n",
    "        invalid_global_gene_tids=(1, 2),\n",
    "        subset_token_ids_np=None,\n",
    "        old_tid_to_vocab_lut=None,\n",
    "        m_tgt: int = 0,\n",
    "        drop_first_gene_token: bool = True,\n",
    "        use_log1p_expr: bool = True,\n",
    "        use_asinh_delta: bool = False,\n",
    "        delta_clip_abs: float = 5.0,\n",
    "        stable_sort_selected_by_gene_id: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.pair_to_locations = pair_to_locations\n",
    "        self.pairs = list(pairs)\n",
    "        self.baseline_global = np.asarray(baseline_global, dtype=np.float32)\n",
    "        self.baseline_by_cellline = baseline_by_cellline or {}\n",
    "        self.drug_to_target_vec_target_only = drug_to_target_vec_target_only\n",
    "        self.drug2id = drug2id\n",
    "        self.drug_to_smiles_np = drug_to_smiles_np\n",
    "        self.cellline2organid = cellline2organid or {}\n",
    "        self.unk_organ_id = int(unk_organ_id)\n",
    "\n",
    "        self.n_genes_full = int(n_genes_full)\n",
    "        self.steps = int(steps)\n",
    "        self.max_seq_len = int(max_seq_len)\n",
    "        self.batch_size = int(batch_size)\n",
    "\n",
    "        self.control_drug = str(control_drug)\n",
    "        self.pad_id = int(pad_id)\n",
    "        self.cls_id = int(cls_id)\n",
    "        self.organtok_id = int(organtok_id)\n",
    "\n",
    "        self.drug_col = drug_col\n",
    "        self.cell_col = cell_col\n",
    "        self.genes_col = genes_col\n",
    "        self.expr_col = expr_col\n",
    "\n",
    "        self.cap_per_pair_in_rg = cap_per_pair_in_rg\n",
    "        self.max_tries_per_pair = int(max_tries_per_pair)\n",
    "        self.seed = int(seed)\n",
    "\n",
    "        any_vec = next(iter(self.drug_to_smiles_np.values()))\n",
    "        self.smiles_dim = int(any_vec.shape[-1])\n",
    "\n",
    "        self.invalid_global_gene_tids = np.asarray(list(set(int(x) for x in invalid_global_gene_tids)), dtype=np.int64)\n",
    "\n",
    "        self.m_tgt = int(m_tgt); assert self.m_tgt > 0\n",
    "        self.drop_first_gene_token = bool(drop_first_gene_token)\n",
    "\n",
    "        self.subset_token_ids_np = subset_token_ids_np\n",
    "        self.old_tid_to_vocab_lut = old_tid_to_vocab_lut\n",
    "\n",
    "        self.use_log1p_expr = bool(use_log1p_expr)\n",
    "        self.use_asinh_delta = bool(use_asinh_delta)\n",
    "        self.delta_clip_abs = float(delta_clip_abs)\n",
    "        self.stable_sort_selected_by_gene_id = bool(stable_sort_selected_by_gene_id)\n",
    "\n",
    "        if pair_weights is None:\n",
    "            self.pair_weights = None\n",
    "        else:\n",
    "            w = np.asarray(pair_weights, dtype=np.float64)\n",
    "            assert len(w) == len(self.pairs)\n",
    "            w = np.clip(w, 0.0, None)\n",
    "            w = w / (w.sum() + 1e-12)\n",
    "            self.pair_weights = w\n",
    "\n",
    "        self._pf_cache = {}\n",
    "\n",
    "    def _get_pf(self, file_path):\n",
    "        pf = self._pf_cache.get(file_path, None)\n",
    "        if pf is None:\n",
    "            pf = pq.ParquetFile(file_path)\n",
    "            self._pf_cache[file_path] = pf\n",
    "        return pf\n",
    "\n",
    "    def _read_row_group_df(self, file_path, rg_id, columns):\n",
    "        pf = self._get_pf(file_path)\n",
    "        return pf.read_row_group(rg_id, columns=columns).to_pandas()\n",
    "\n",
    "    def _scale_delta(self, delta: np.ndarray) -> np.ndarray:\n",
    "        if self.delta_clip_abs and self.delta_clip_abs > 0:\n",
    "            delta = np.clip(delta, -self.delta_clip_abs, self.delta_clip_abs)\n",
    "        if self.use_asinh_delta:\n",
    "            delta = np.arcsinh(delta)\n",
    "        return delta.astype(np.float32, copy=False)\n",
    "\n",
    "    def _prepare_sparse_sorted_drop0(self, genes, expr):\n",
    "        if genes is None or expr is None:\n",
    "            return np.asarray([], dtype=np.int64), np.asarray([], dtype=np.float32)\n",
    "\n",
    "        idx = np.asarray(genes, dtype=np.int64)\n",
    "        val = np.asarray(expr, dtype=np.float32)\n",
    "        L = min(idx.size, val.size)\n",
    "        idx = idx[:L]; val = val[:L]\n",
    "\n",
    "        if self.drop_first_gene_token and L >= 1:\n",
    "            idx = idx[1:]\n",
    "            val = val[1:]\n",
    "\n",
    "        if idx.size == 0:\n",
    "            return idx, val\n",
    "\n",
    "        # Variant A: log1p before delta\n",
    "        if self.use_log1p_expr:\n",
    "            val = np.log1p(np.clip(val, a_min=0.0, a_max=None))\n",
    "\n",
    "        if self.invalid_global_gene_tids.size > 0:\n",
    "            m_bad = np.isin(idx, self.invalid_global_gene_tids, assume_unique=False)\n",
    "            if m_bad.any():\n",
    "                keep = ~m_bad\n",
    "                idx = idx[keep]; val = val[keep]\n",
    "                if idx.size == 0:\n",
    "                    return idx, val\n",
    "\n",
    "        m = (idx >= 0) & (idx < self.n_genes_full)\n",
    "        idx = idx[m]; val = val[m]\n",
    "        if idx.size == 0:\n",
    "            return idx, val\n",
    "\n",
    "        order = np.argsort(idx)\n",
    "        return idx[order], val[order]\n",
    "\n",
    "    def _fill_one_row(self, row_genes, row_expr, baseline_vec, input_ids_row, values_row, attn_row):\n",
    "        idx_sorted, val_sorted = self._prepare_sparse_sorted_drop0(row_genes, row_expr)\n",
    "        if idx_sorted.size == 0:\n",
    "            return False\n",
    "\n",
    "        delta = val_sorted - baseline_vec[idx_sorted]\n",
    "        delta = self._scale_delta(delta)\n",
    "\n",
    "        mask_sub = np.isin(idx_sorted, self.subset_token_ids_np, assume_unique=False)\n",
    "        if not mask_sub.any():\n",
    "            return False\n",
    "\n",
    "        idx_sub = idx_sorted[mask_sub]\n",
    "        del_sub = delta[mask_sub]\n",
    "        if idx_sub.size == 0:\n",
    "            return False\n",
    "\n",
    "        k = min(self.max_seq_len, idx_sub.size)\n",
    "        top = np.argpartition(-np.abs(del_sub), k - 1)[:k]\n",
    "        sel_tid = idx_sub[top]\n",
    "        sel_del = del_sub[top]\n",
    "\n",
    "        # stable order: sort by gene_id (deterministic)\n",
    "        if self.stable_sort_selected_by_gene_id:\n",
    "            o2 = np.argsort(sel_tid)\n",
    "            sel_tid = sel_tid[o2]\n",
    "            sel_del = sel_del[o2]\n",
    "\n",
    "        sel_vid = self.old_tid_to_vocab_lut[sel_tid]\n",
    "        ok = sel_vid != -1\n",
    "        if not ok.any():\n",
    "            return False\n",
    "\n",
    "        sel_vid = sel_vid[ok]\n",
    "        sel_del = sel_del[ok]\n",
    "\n",
    "        L = min(self.max_seq_len, sel_vid.size)\n",
    "        if L <= 0:\n",
    "            return False\n",
    "\n",
    "        # layout: [CLS][ORGAN] + genes...\n",
    "        input_ids_row[2:2+L] = sel_vid[:L]\n",
    "        values_row[2:2+L]    = sel_del[:L]\n",
    "        attn_row[2:2+L]      = 1\n",
    "        return True\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        base_seed = self.seed if worker_info is None else (self.seed + worker_info.id)\n",
    "        rng = np.random.default_rng(base_seed)\n",
    "\n",
    "        pairs = self.pairs\n",
    "        weights = self.pair_weights\n",
    "        n_pairs = len(pairs)\n",
    "\n",
    "        cols = [self.drug_col, self.cell_col, self.genes_col, self.expr_col]\n",
    "        seq_len = 2 + self.max_seq_len\n",
    "\n",
    "        cnt = 0\n",
    "        while True:\n",
    "            chosen = []\n",
    "            seen_drugs = set()\n",
    "\n",
    "            tries = 0\n",
    "            while len(chosen) < self.batch_size and tries < 80:\n",
    "                tries += 1\n",
    "                draw = min(max(self.batch_size * 8, 512), max(n_pairs, 1))\n",
    "                if weights is None:\n",
    "                    cand_idx = rng.integers(0, n_pairs, size=draw)\n",
    "                else:\n",
    "                    cand_idx = rng.choice(n_pairs, size=draw, replace=True, p=weights)\n",
    "\n",
    "                for ii in cand_idx:\n",
    "                    drug_name, cell_line = pairs[int(ii)]\n",
    "                    drug_name = str(drug_name); cell_line = str(cell_line)\n",
    "\n",
    "                    if drug_name == self.control_drug:\n",
    "                        continue\n",
    "                    if drug_name in seen_drugs:\n",
    "                        continue\n",
    "\n",
    "                    y_vec = self.drug_to_target_vec_target_only.get(drug_name, None)\n",
    "                    if y_vec is None or float(y_vec.sum()) <= 0.0:\n",
    "                        continue\n",
    "\n",
    "                    sm = self.drug_to_smiles_np.get(drug_name, None)\n",
    "                    if sm is None or (not np.isfinite(sm).all()) or (np.abs(sm).sum() == 0.0):\n",
    "                        continue\n",
    "\n",
    "                    if not self.pair_to_locations.get((drug_name, cell_line), []):\n",
    "                        continue\n",
    "\n",
    "                    chosen.append((drug_name, cell_line))\n",
    "                    seen_drugs.add(drug_name)\n",
    "                    if len(chosen) >= self.batch_size:\n",
    "                        break\n",
    "\n",
    "            if len(chosen) < self.batch_size:\n",
    "                continue\n",
    "\n",
    "            input_ids = np.full((self.batch_size, seq_len), self.pad_id, dtype=np.int64)\n",
    "            values    = np.zeros((self.batch_size, seq_len), dtype=np.float32)\n",
    "            attn      = np.zeros((self.batch_size, seq_len), dtype=np.int64)\n",
    "\n",
    "            input_ids[:, 0] = self.cls_id\n",
    "            input_ids[:, 1] = self.organtok_id\n",
    "            attn[:, 0:2] = 1\n",
    "\n",
    "            y_batch       = np.zeros((self.batch_size, self.m_tgt), dtype=np.float32)\n",
    "            smiles_batch  = np.zeros((self.batch_size, self.smiles_dim), dtype=np.float32)\n",
    "            drug_id_batch = np.zeros((self.batch_size,), dtype=np.int64)\n",
    "            organ_id_batch = np.zeros((self.batch_size,), dtype=np.int64)\n",
    "\n",
    "            row_ptr = 0\n",
    "            built_any = False\n",
    "\n",
    "            for (drug_name, cell_line) in chosen:\n",
    "                locs = self.pair_to_locations.get((drug_name, cell_line), [])\n",
    "                if not locs:\n",
    "                    continue\n",
    "\n",
    "                baseline = self.baseline_by_cellline.get(cell_line, self.baseline_global)\n",
    "                did = int(self.drug2id.get(drug_name, 0))\n",
    "                oid = int(self.cellline2organid.get(cell_line, self.unk_organ_id))\n",
    "\n",
    "                y_vec = self.drug_to_target_vec_target_only[drug_name]\n",
    "                sm_vec = self.drug_to_smiles_np[drug_name]\n",
    "\n",
    "                ok_row = False\n",
    "                for _ in range(self.max_tries_per_pair):\n",
    "                    fpath, rg_id = locs[rng.integers(0, len(locs))]\n",
    "                    df = self._read_row_group_df(fpath, rg_id, columns=cols)\n",
    "\n",
    "                    df = df[(df[self.drug_col].astype(str) == drug_name) &\n",
    "                            (df[self.cell_col].astype(str) == cell_line)]\n",
    "                    if len(df) == 0:\n",
    "                        continue\n",
    "\n",
    "                    r = df.sample(1, random_state=None).itertuples(index=False).__next__()\n",
    "\n",
    "                    ok_row = self._fill_one_row(\n",
    "                        getattr(r, self.genes_col),\n",
    "                        getattr(r, self.expr_col),\n",
    "                        baseline,\n",
    "                        input_ids[row_ptr], values[row_ptr], attn[row_ptr]\n",
    "                    )\n",
    "                    if ok_row:\n",
    "                        y_batch[row_ptr] = y_vec\n",
    "                        smiles_batch[row_ptr] = sm_vec\n",
    "                        drug_id_batch[row_ptr] = did\n",
    "                        organ_id_batch[row_ptr] = oid\n",
    "                        row_ptr += 1\n",
    "                        built_any = True\n",
    "                        break  # ✅ break only if ok_row\n",
    "\n",
    "                if row_ptr >= self.batch_size:\n",
    "                    break\n",
    "\n",
    "            if not built_any:\n",
    "                continue\n",
    "\n",
    "            if row_ptr < self.batch_size:\n",
    "                fill = self.batch_size - row_ptr\n",
    "                input_ids[row_ptr:]      = input_ids[:fill]\n",
    "                values[row_ptr:]         = values[:fill]\n",
    "                attn[row_ptr:]           = attn[:fill]\n",
    "                y_batch[row_ptr:]        = y_batch[:fill]\n",
    "                smiles_batch[row_ptr:]   = smiles_batch[:fill]\n",
    "                drug_id_batch[row_ptr:]  = drug_id_batch[:fill]\n",
    "                organ_id_batch[row_ptr:] = organ_id_batch[:fill]\n",
    "\n",
    "            yield {\n",
    "                \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "                \"values\": torch.tensor(values, dtype=torch.float32),\n",
    "                \"attention_mask\": torch.tensor(attn, dtype=torch.long),\n",
    "                \"y_targets\": torch.tensor(y_batch, dtype=torch.float32),\n",
    "                \"smiles_emb\": torch.tensor(smiles_batch, dtype=torch.float32),\n",
    "                \"drug_id\": torch.tensor(drug_id_batch, dtype=torch.long),\n",
    "                \"organ_id\": torch.tensor(organ_id_batch, dtype=torch.long),\n",
    "            }\n",
    "\n",
    "            cnt += 1\n",
    "            if cnt >= self.steps:\n",
    "                return\n",
    "\n",
    "\n",
    "train_ds = TahoeFPParquetDataset_UniqueDrug(\n",
    "    pair_to_locations=pair_to_locations,\n",
    "    pairs=train_pairs,\n",
    "    baseline_global=baseline_global,\n",
    "    baseline_by_cellline=baseline_by_cl,\n",
    "    drug_to_target_vec_target_only=drug_to_target_vec_tgt,\n",
    "    drug2id=drug2id,\n",
    "    drug_to_smiles_np=drug_to_smiles_np,\n",
    "    cellline2organid=cellline2organid,\n",
    "    unk_organ_id=UNK_ORGAN_ID,\n",
    "    n_genes_full=N_GENES,\n",
    "    steps=STEPS_PER_EPOCH,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    control_drug=CONTROL_DRUG,\n",
    "    pad_id=PAD_ID,\n",
    "    cls_id=CLS_ID,\n",
    "    organtok_id=ORGAN_TOK_ID,\n",
    "    pair_weights=w_train,\n",
    "    seed=SEED,\n",
    "    subset_token_ids_np=subset_token_ids_np,\n",
    "    old_tid_to_vocab_lut=old_tid_to_vocab_lut,\n",
    "    m_tgt=M_TGT,\n",
    "    drop_first_gene_token=DROP_FIRST_GENE_TOKEN,\n",
    "    use_log1p_expr=USE_LOG1P_EXPR,\n",
    "    use_asinh_delta=USE_ASINH_DELTA,\n",
    "    delta_clip_abs=DELTA_CLIP_ABS,\n",
    "    stable_sort_selected_by_gene_id=True,\n",
    ")\n",
    "\n",
    "val_ds = TahoeFPParquetDataset_UniqueDrug(\n",
    "    pair_to_locations=pair_to_locations,\n",
    "    pairs=val_pairs,\n",
    "    baseline_global=baseline_global,\n",
    "    baseline_by_cellline=baseline_by_cl,\n",
    "    drug_to_target_vec_target_only=drug_to_target_vec_tgt,\n",
    "    drug2id=drug2id,\n",
    "    drug_to_smiles_np=drug_to_smiles_np,\n",
    "    cellline2organid=cellline2organid,\n",
    "    unk_organ_id=UNK_ORGAN_ID,\n",
    "    n_genes_full=N_GENES,\n",
    "    steps=VAL_STEPS,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    control_drug=CONTROL_DRUG,\n",
    "    pad_id=PAD_ID,\n",
    "    cls_id=CLS_ID,\n",
    "    organtok_id=ORGAN_TOK_ID,\n",
    "    pair_weights=w_val,\n",
    "    seed=SEED + 123,\n",
    "    subset_token_ids_np=subset_token_ids_np,\n",
    "    old_tid_to_vocab_lut=old_tid_to_vocab_lut,\n",
    "    m_tgt=M_TGT,\n",
    "    drop_first_gene_token=DROP_FIRST_GENE_TOKEN,\n",
    "    use_log1p_expr=USE_LOG1P_EXPR,\n",
    "    use_asinh_delta=USE_ASINH_DELTA,\n",
    "    delta_clip_abs=DELTA_CLIP_ABS,\n",
    "    stable_sort_selected_by_gene_id=True,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=None,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=(NUM_WORKERS > 0),\n",
    "    prefetch_factor=2 if NUM_WORKERS > 0 else None,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=None,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 12) Model: ORGAN embedding + NO positional emb + learnable CLIP temperature\n",
    "# =========================================================\n",
    "class FPEncoderWithOrgan(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, num_layers, pad_id,\n",
    "                 max_len: int, num_organs: int, organ_pos: int = 1, use_pos_emb: bool = False):\n",
    "        super().__init__()\n",
    "        self.token_emb  = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
    "        self.value_proj = nn.Linear(1, d_model)\n",
    "\n",
    "        self.use_pos_emb = bool(use_pos_emb)\n",
    "        if self.use_pos_emb:\n",
    "            self.pos_emb = nn.Embedding(max_len, d_model)\n",
    "\n",
    "        self.organ_emb = nn.Embedding(num_organs, d_model)\n",
    "        self.organ_pos = int(organ_pos)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads, dim_feedforward=4*d_model,\n",
    "            dropout=0.1, batch_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, input_ids, values, attention_mask, organ_id):\n",
    "        B, L = input_ids.shape\n",
    "        dev = input_ids.device\n",
    "\n",
    "        x = self.token_emb(input_ids) + self.value_proj(values.unsqueeze(-1))\n",
    "\n",
    "        if self.use_pos_emb:\n",
    "            pos = torch.arange(L, device=dev).unsqueeze(0).expand(B, L)\n",
    "            x = x + self.pos_emb(pos)\n",
    "\n",
    "        if organ_id is not None:\n",
    "            x[:, self.organ_pos, :] = x[:, self.organ_pos, :] + self.organ_emb(organ_id.to(dev)).to(x.dtype)\n",
    "\n",
    "        key_padding_mask = (attention_mask == 0)\n",
    "        h = self.encoder(x, src_key_padding_mask=key_padding_mask)\n",
    "        return h[:, 0, :]\n",
    "\n",
    "\n",
    "class FPModelTied_OrganCLIP(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, num_layers, pad_id, smiles_dim,\n",
    "                 max_len: int, num_organs: int, n_special: int, tau_init: float = 0.10):\n",
    "        super().__init__()\n",
    "        self.n_special = int(n_special)\n",
    "\n",
    "        self.encoder = FPEncoderWithOrgan(\n",
    "            vocab_size=vocab_size,\n",
    "            d_model=d_model,\n",
    "            n_heads=n_heads,\n",
    "            num_layers=num_layers,\n",
    "            pad_id=pad_id,\n",
    "            max_len=max_len,\n",
    "            num_organs=num_organs,\n",
    "            organ_pos=1,          # [CLS][ORGAN]\n",
    "            use_pos_emb=False,   \n",
    "        )\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.smiles_head = nn.Sequential(\n",
    "            nn.Linear(d_model, 4*d_model),\n",
    "            nn.BatchNorm1d(4*d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(4*d_model, smiles_dim),\n",
    "        )\n",
    "\n",
    "        # CLIP logit scale: logits = (z1 @ z2.T) * exp(logit_scale)\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1.0 / float(tau_init)))\n",
    "\n",
    "    def gene_emb_subset(self):\n",
    "        return self.encoder.token_emb.weight[self.n_special:, :] \n",
    "\n",
    "    def get_tau(self):\n",
    "        # tau = 1/exp(scale)\n",
    "        return (1.0 / self.logit_scale.exp()).clamp(0.01, 0.5)\n",
    "\n",
    "    def forward(self, input_ids, values, attention_mask, organ_id, return_smiles=False):\n",
    "        h_cls = self.encoder(input_ids, values, attention_mask, organ_id=organ_id)\n",
    "        v_pred = self.proj(h_cls)\n",
    "        z_pred = self.smiles_head(h_cls)\n",
    "        if return_smiles:\n",
    "            return v_pred, z_pred\n",
    "        return v_pred\n",
    "\n",
    "\n",
    "D_MODEL = 256\n",
    "N_HEADS = 8\n",
    "N_LAYERS = 4\n",
    "\n",
    "model = FPModelTied_OrganCLIP(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    d_model=D_MODEL,\n",
    "    n_heads=N_HEADS,\n",
    "    num_layers=N_LAYERS,\n",
    "    pad_id=PAD_ID,\n",
    "    smiles_dim=SMILES_DIM,\n",
    "    max_len=(2 + MAX_SEQ_LEN),\n",
    "    num_organs=NUM_ORGANS,\n",
    "    n_special=N_SPECIAL,\n",
    "    tau_init=TAU_INIT,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 13) Load pretrained gene embeddings into subset token emb\n",
    "# =========================================================\n",
    "def load_pretrained_subset_into_token_emb(token_emb: nn.Embedding, npy_path: str, device):\n",
    "    if (npy_path is None) or (not os.path.exists(npy_path)):\n",
    "        print(\"⚠️ PRETRAINED_GENE_NPY not found. Skip loading.\")\n",
    "        return\n",
    "    W = np.load(npy_path)  # (N_GENES, d)\n",
    "    Wt = torch.tensor(W, dtype=torch.float32, device=device)\n",
    "    d = token_emb.weight.shape[1]\n",
    "    if Wt.shape[1] != d:\n",
    "        raise ValueError(f\"d mismatch: npy={Wt.shape[1]} vs token_emb={d}\")\n",
    "\n",
    "    loaded = 0\n",
    "    with torch.no_grad():\n",
    "        for sid, old_tid in enumerate(subset_token_ids):\n",
    "            vid = N_SPECIAL + sid\n",
    "            if 0 <= old_tid < Wt.shape[0]:\n",
    "                token_emb.weight[vid].copy_(Wt[int(old_tid)])\n",
    "                loaded += 1\n",
    "    print(f\"✅ token_emb loaded: {loaded}/{len(subset_token_ids)}\")\n",
    "\n",
    "load_pretrained_subset_into_token_emb(model.encoder.token_emb, PRETRAINED_GENE_NPY, device=device)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 14) target_sub_ids\n",
    "# =========================================================\n",
    "target_sub_ids = torch.tensor([old_tid_to_subid[tid] for tid in target_token_ids],\n",
    "                              dtype=torch.long, device=device)\n",
    "print(\"[target_sub_ids]:\", tuple(target_sub_ids.shape))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 15) pos_weight (TARGET-ONLY) \n",
    "# =========================================================\n",
    "pos_weight = torch.ones((M_TGT,), dtype=torch.float32, device=device)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 16) Losses (Targets + SMILES CLIP)\n",
    "# =========================================================\n",
    "def info_nce_ranking_loss_multi_pos(\n",
    "    v_pred: torch.Tensor,\n",
    "    gene_emb: torch.Tensor,\n",
    "    y_targets: torch.Tensor,\n",
    "    num_neg: int = 256,\n",
    "    num_pos: int = 8,\n",
    "    tau: float = 0.1,\n",
    "):\n",
    "    device_ = v_pred.device\n",
    "    B, _ = v_pred.shape\n",
    "    losses = []\n",
    "\n",
    "    v_pred = F.normalize(v_pred, dim=1)\n",
    "    gene_emb = F.normalize(gene_emb, dim=1)\n",
    "\n",
    "    for i in range(B):\n",
    "        pos_idx = (y_targets[i] > 0.5).nonzero(as_tuple=True)[0]\n",
    "        if pos_idx.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        neg_idx_all = (y_targets[i] < 0.5).nonzero(as_tuple=True)[0]\n",
    "        if neg_idx_all.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        if num_pos and pos_idx.numel() > num_pos:\n",
    "            pos_idx = pos_idx[torch.randperm(pos_idx.numel(), device=device_)[:num_pos]]\n",
    "\n",
    "        if neg_idx_all.numel() > num_neg:\n",
    "            neg_idx = neg_idx_all[torch.randperm(neg_idx_all.numel(), device=device_)[:num_neg]]\n",
    "        else:\n",
    "            neg_idx = neg_idx_all\n",
    "\n",
    "        pos_emb = gene_emb[pos_idx]\n",
    "        neg_emb = gene_emb[neg_idx]\n",
    "        cand_emb = torch.cat([pos_emb, neg_emb], dim=0)\n",
    "\n",
    "        v = v_pred[i].unsqueeze(0)\n",
    "        scores = (v @ cand_emb.T).squeeze(0) / tau\n",
    "\n",
    "        P = pos_emb.size(0)\n",
    "        logits = scores.unsqueeze(0).repeat(P, 1)\n",
    "        targets = torch.arange(P, device=device_, dtype=torch.long)\n",
    "        losses.append(F.cross_entropy(logits, targets))\n",
    "\n",
    "    if len(losses) == 0:\n",
    "        return torch.tensor(0.0, device=device_)\n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "\n",
    "def bce_with_neg_sampling_cosine(\n",
    "    pred_vec: torch.Tensor,\n",
    "    y_targets: torch.Tensor,\n",
    "    gene_emb: torch.Tensor,\n",
    "    pos_weight_full: torch.Tensor,\n",
    "    num_neg: int = 2048,\n",
    "    pos_cap: int | None = None,\n",
    "    tau_bce: float = 0.10,\n",
    "):\n",
    "    device_ = pred_vec.device\n",
    "    B, _ = pred_vec.shape\n",
    "    losses = []\n",
    "\n",
    "    gene_emb = F.normalize(gene_emb, dim=1)\n",
    "\n",
    "    for i in range(B):\n",
    "        yi = y_targets[i]\n",
    "        pos_idx = (yi > 0.5).nonzero(as_tuple=True)[0]\n",
    "        if pos_idx.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        if (pos_cap is not None) and (pos_idx.numel() > pos_cap):\n",
    "            pos_idx = pos_idx[torch.randperm(pos_idx.numel(), device=device_)[:pos_cap]]\n",
    "\n",
    "        neg_idx_all = (yi < 0.5).nonzero(as_tuple=True)[0]\n",
    "        if neg_idx_all.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        k = min(int(num_neg), neg_idx_all.numel())\n",
    "        neg_idx = neg_idx_all[torch.randperm(neg_idx_all.numel(), device=device_)[:k]]\n",
    "\n",
    "        idx = torch.cat([pos_idx, neg_idx], dim=0)\n",
    "\n",
    "        v = F.normalize(pred_vec[i], dim=0)\n",
    "        logits = (v @ gene_emb[idx].T) / tau_bce\n",
    "\n",
    "        y_sub = yi[idx]\n",
    "        pw_sub = pos_weight_full[idx]\n",
    "\n",
    "        losses.append(F.binary_cross_entropy_with_logits(logits, y_sub, pos_weight=pw_sub, reduction=\"mean\"))\n",
    "\n",
    "    if len(losses) == 0:\n",
    "        return torch.tensor(0.0, device=device_)\n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "\n",
    "def combined_target_loss_neg_sampling_tied(\n",
    "    pred_vec: torch.Tensor,\n",
    "    y_targets: torch.Tensor,\n",
    "    gene_emb: torch.Tensor,\n",
    "    pos_weight: torch.Tensor,\n",
    "    lambda_cos: float = 1.0,\n",
    "    lambda_bce: float = 0.1,\n",
    "    lambda_rank: float = 0.5,\n",
    "    bce_num_neg: int = 2048,\n",
    "    bce_pos_cap: int | None = None,\n",
    "    rank_num_neg: int = 256,\n",
    "    rank_num_pos: int = 8,\n",
    "    tau_rank: float = 0.1,\n",
    "    tau_bce: float = 0.10,\n",
    "):\n",
    "    device_ = pred_vec.device\n",
    "\n",
    "    gene_emb_norm = F.normalize(gene_emb, dim=1)\n",
    "    pred_norm = F.normalize(pred_vec, dim=1)\n",
    "\n",
    "    true_vec = y_targets @ gene_emb_norm\n",
    "    num_t = y_targets.sum(dim=1, keepdim=True)\n",
    "    mask = (num_t > 0).squeeze(1)\n",
    "\n",
    "    if mask.any():\n",
    "        true_vec_pos = true_vec[mask] / (num_t[mask] + 1e-6)\n",
    "        true_vec_pos = F.normalize(true_vec_pos, dim=1)\n",
    "        pred_pos = pred_norm[mask]\n",
    "        loss_cos = 1.0 - (pred_pos * true_vec_pos).sum(dim=1).mean()\n",
    "    else:\n",
    "        loss_cos = torch.tensor(0.0, device=device_)\n",
    "\n",
    "    loss_bce = bce_with_neg_sampling_cosine(\n",
    "        pred_vec=pred_vec,\n",
    "        y_targets=y_targets,\n",
    "        gene_emb=gene_emb,\n",
    "        pos_weight_full=pos_weight,\n",
    "        num_neg=bce_num_neg,\n",
    "        pos_cap=bce_pos_cap,\n",
    "        tau_bce=tau_bce,\n",
    "    )\n",
    "\n",
    "    loss_rank = info_nce_ranking_loss_multi_pos(\n",
    "        v_pred=pred_vec,\n",
    "        gene_emb=gene_emb,\n",
    "        y_targets=y_targets,\n",
    "        num_neg=rank_num_neg,\n",
    "        num_pos=rank_num_pos,\n",
    "        tau=tau_rank,\n",
    "    )\n",
    "\n",
    "    loss = lambda_cos * loss_cos + lambda_bce * loss_bce + lambda_rank * loss_rank\n",
    "    return loss, loss_cos.detach(), loss_bce.detach(), loss_rank.detach()\n",
    "\n",
    "\n",
    "# --- SMILES CLIP loss (+ optional cosine align) ---\n",
    "def clip_loss(z_pred: torch.Tensor, z_true: torch.Tensor, tau: torch.Tensor):\n",
    "    z1 = F.normalize(z_pred, dim=1)\n",
    "    z2 = F.normalize(z_true, dim=1)\n",
    "    logits = (z1 @ z2.T) / tau\n",
    "    labels = torch.arange(z_pred.size(0), device=z_pred.device, dtype=torch.long)\n",
    "    return 0.5 * (F.cross_entropy(logits, labels) + F.cross_entropy(logits.T, labels))\n",
    "\n",
    "def smiles_align_loss_cosine(z_pred: torch.Tensor, z_true: torch.Tensor):\n",
    "    z1 = F.normalize(z_pred, dim=1)\n",
    "    z2 = F.normalize(z_true, dim=1)\n",
    "    return 1.0 - (z1 * z2).sum(dim=1).mean()\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 17) Eval\n",
    "# =========================================================\n",
    "def compute_recall_precision_at_k(scores: torch.Tensor, y_true: torch.Tensor, k: int = 20):\n",
    "    B, M = scores.shape\n",
    "    kk = min(k, M)\n",
    "    _, topk_idx = torch.topk(scores, k=kk, dim=1)\n",
    "\n",
    "    recalls, precisions = [], []\n",
    "    for i in range(B):\n",
    "        true_labels = y_true[i]\n",
    "        num_pos_ = true_labels.sum().item()\n",
    "        if num_pos_ == 0:\n",
    "            continue\n",
    "        topk = topk_idx[i]\n",
    "        num_pos_in_topk = true_labels[topk].sum().item()\n",
    "        recalls.append(num_pos_in_topk / max(num_pos_, 1e-6))\n",
    "        precisions.append(num_pos_in_topk / max(kk, 1))\n",
    "\n",
    "    if len(recalls) == 0:\n",
    "        return 0.0, 0.0\n",
    "    return float(sum(recalls) / len(recalls)), float(sum(precisions) / len(precisions))\n",
    "\n",
    "@torch.no_grad()\n",
    "def smiles_retrieval_hitk(z_pred: torch.Tensor, drug_id: torch.Tensor, smiles_bank_t: torch.Tensor, k_list=(1,5,10)):\n",
    "    z = F.normalize(z_pred.float(), dim=1)\n",
    "    b = F.normalize(smiles_bank_t.float(), dim=1)\n",
    "    logits = z @ b.T\n",
    "    out = {}\n",
    "    for k in k_list:\n",
    "        topk = torch.topk(logits, k=min(k, logits.size(1)), dim=1).indices\n",
    "        hit = (topk == drug_id.view(-1,1)).any(dim=1).float().mean().item()\n",
    "        out[f\"Hit@{k}\"] = float(hit)\n",
    "    true_vec = b[drug_id]\n",
    "    out[\"TrueCos\"] = float((z * true_vec).sum(dim=1).mean().item())\n",
    "    return out\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_fp(model, loader, device, target_sub_ids, smiles_bank_t, k_list=(5,10), hitk=(1,5,10)):\n",
    "    model.eval()\n",
    "\n",
    "    gene_emb = model.gene_emb_subset()[target_sub_ids].to(device)\n",
    "    g_norm = F.normalize(gene_emb, dim=1)\n",
    "\n",
    "    recall_sums = {k: 0.0 for k in k_list}\n",
    "    prec_sums   = {k: 0.0 for k in k_list}\n",
    "    counts_     = {k: 0   for k in k_list}\n",
    "\n",
    "    hit_sums = {f\"Hit@{k}\": 0.0 for k in hitk}\n",
    "    hit_sums[\"TrueCos\"] = 0.0\n",
    "    clip_sum = 0.0\n",
    "    tau_sum  = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "        values    = batch[\"values\"].to(device, non_blocking=True)\n",
    "        attn      = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "        y_targets = batch[\"y_targets\"].to(device, non_blocking=True)\n",
    "        z_true    = batch[\"smiles_emb\"].to(device, non_blocking=True)\n",
    "        drug_id   = batch[\"drug_id\"].to(device, non_blocking=True)\n",
    "        organ_id  = batch[\"organ_id\"].to(device, non_blocking=True)\n",
    "\n",
    "        v_pred, z_pred = model(input_ids, values, attn, organ_id=organ_id, return_smiles=True)\n",
    "        v_norm = F.normalize(v_pred, dim=1)\n",
    "        scores = v_norm @ g_norm.T\n",
    "\n",
    "        for k in k_list:\n",
    "            r, p = compute_recall_precision_at_k(scores, y_targets, k=k)\n",
    "            recall_sums[k] += r\n",
    "            prec_sums[k]   += p\n",
    "            counts_[k]     += 1\n",
    "\n",
    "        bs = input_ids.size(0)\n",
    "        m = smiles_retrieval_hitk(z_pred, drug_id, smiles_bank_t, k_list=hitk)\n",
    "        for k in hitk:\n",
    "            hit_sums[f\"Hit@{k}\"] += m[f\"Hit@{k}\"] * bs\n",
    "        hit_sums[\"TrueCos\"] += m[\"TrueCos\"] * bs\n",
    "\n",
    "        tau = model.get_tau()\n",
    "        clip_sum += float(clip_loss(z_pred, z_true, tau=tau).item()) * bs\n",
    "        tau_sum  += float(tau.item()) * bs\n",
    "\n",
    "        n += bs\n",
    "\n",
    "    out = {}\n",
    "    for k in k_list:\n",
    "        out[f\"Recall@{k}\"] = recall_sums[k] / max(counts_[k], 1)\n",
    "        out[f\"Precision@{k}\"] = prec_sums[k] / max(counts_[k], 1)\n",
    "\n",
    "    for k in hitk:\n",
    "        out[f\"SMILES_Hit@{k}\"] = hit_sums[f\"Hit@{k}\"] / max(n, 1)\n",
    "    out[\"SMILES_TrueCos\"] = hit_sums[\"TrueCos\"] / max(n, 1)\n",
    "    out[\"SMILES_CLIP\"] = clip_sum / max(n, 1)\n",
    "    out[\"tau\"] = tau_sum / max(n, 1)\n",
    "    return out\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 18) Train loop (OneCycleLR fixed + grad accumulation + AMP)\n",
    "# =========================================================\n",
    "def infinite_loader(loader):\n",
    "    while True:\n",
    "        for b in loader:\n",
    "            yield b\n",
    "\n",
    "USE_AMP = (device.type == \"cuda\")\n",
    "scaler = torch.amp.GradScaler(\"cuda\", enabled=USE_AMP)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "updates_per_epoch = max(1, STEPS_PER_EPOCH // max(1, ACCUM_STEPS))\n",
    "total_updates = EPOCHS * updates_per_epoch\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=LR,\n",
    "    total_steps=total_updates,\n",
    "    pct_start=0.05,\n",
    "    anneal_strategy=\"cos\",\n",
    "    div_factor=10.0,\n",
    "    final_div_factor=100.0,\n",
    ")\n",
    "\n",
    "smiles_bank_t = torch.tensor(smiles_bank_np, dtype=torch.float32, device=device)\n",
    "\n",
    "def train_one_epoch_fixed_steps(\n",
    "    model,\n",
    "    train_loader,\n",
    "    device,\n",
    "    steps_per_epoch,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    scaler,\n",
    "    target_sub_ids,\n",
    "    pos_weight,\n",
    "    smiles_bank_t,\n",
    "    log_every=50,\n",
    "    grad_clip=1.0,\n",
    "    accum_steps=1,\n",
    "):\n",
    "    model.train()\n",
    "    it = infinite_loader(train_loader)\n",
    "\n",
    "    run_total = 0.0\n",
    "    run_tgt   = 0.0\n",
    "    run_clip  = 0.0\n",
    "    run_align = 0.0\n",
    "    run_rank_last = 0.0\n",
    "\n",
    "    run_hit5 = 0.0\n",
    "    run_truecos = 0.0\n",
    "    n = 0\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    update_count = 0\n",
    "\n",
    "    pbar = tqdm(range(1, steps_per_epoch + 1), desc=\"Train\", leave=True, dynamic_ncols=True)\n",
    "\n",
    "    for step in pbar:\n",
    "        batch = next(it)\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "        values    = batch[\"values\"].to(device, non_blocking=True)\n",
    "        attn      = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "        y_targets = batch[\"y_targets\"].to(device, non_blocking=True)\n",
    "        z_true    = batch[\"smiles_emb\"].to(device, non_blocking=True)\n",
    "        drug_id   = batch[\"drug_id\"].to(device, non_blocking=True)\n",
    "        organ_id  = batch[\"organ_id\"].to(device, non_blocking=True)\n",
    "\n",
    "        bs = input_ids.size(0)\n",
    "        n += bs\n",
    "\n",
    "        if USE_AMP:\n",
    "            with torch.amp.autocast(\"cuda\", enabled=True):\n",
    "                v_pred, z_pred = model(input_ids, values, attn, organ_id=organ_id, return_smiles=True)\n",
    "\n",
    "                # --- Targets ---\n",
    "                gene_emb = model.gene_emb_subset()[target_sub_ids]  # (M_TGT, d)\n",
    "                loss_targets, loss_cos_t, loss_bce_t, loss_rank_t = combined_target_loss_neg_sampling_tied(\n",
    "                    pred_vec=v_pred,\n",
    "                    y_targets=y_targets,\n",
    "                    gene_emb=gene_emb,\n",
    "                    pos_weight=pos_weight,\n",
    "                    lambda_cos=lambda_cos,\n",
    "                    lambda_bce=lambda_bce,\n",
    "                    lambda_rank=lambda_rank,\n",
    "                    bce_num_neg=bce_num_neg,\n",
    "                    bce_pos_cap=bce_pos_cap,\n",
    "                    rank_num_neg=rank_num_neg,\n",
    "                    rank_num_pos=rank_num_pos,\n",
    "                    tau_rank=tau_rank,\n",
    "                    tau_bce=tau_bce,\n",
    "                )\n",
    "\n",
    "                # --- SMILES CLIP ---\n",
    "                tau = model.get_tau()\n",
    "                loss_c = clip_loss(z_pred, z_true, tau=tau)\n",
    "                loss_a = smiles_align_loss_cosine(z_pred, z_true)\n",
    "                loss_smiles = loss_c + alpha_align * loss_a\n",
    "\n",
    "                loss = (lambda_targets * loss_targets + lambda_smiles * loss_smiles) / float(accum_steps)\n",
    "\n",
    "            if not torch.isfinite(loss).all():\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                continue\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            do_update = (step % accum_steps) == 0\n",
    "            if do_update:\n",
    "                scaler.unscale_(optimizer)\n",
    "                if grad_clip is not None and grad_clip > 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                scheduler.step()\n",
    "                update_count += 1\n",
    "\n",
    "        else:\n",
    "            v_pred, z_pred = model(input_ids, values, attn, organ_id=organ_id, return_smiles=True)\n",
    "\n",
    "            gene_emb = model.gene_emb_subset()[target_sub_ids]\n",
    "            loss_targets, loss_cos_t, loss_bce_t, loss_rank_t = combined_target_loss_neg_sampling_tied(\n",
    "                pred_vec=v_pred,\n",
    "                y_targets=y_targets,\n",
    "                gene_emb=gene_emb,\n",
    "                pos_weight=pos_weight,\n",
    "                lambda_cos=lambda_cos,\n",
    "                lambda_bce=lambda_bce,\n",
    "                lambda_rank=lambda_rank,\n",
    "                bce_num_neg=bce_num_neg,\n",
    "                bce_pos_cap=bce_pos_cap,\n",
    "                rank_num_neg=rank_num_neg,\n",
    "                rank_num_pos=rank_num_pos,\n",
    "                tau_rank=tau_rank,\n",
    "                tau_bce=tau_bce,\n",
    "            )\n",
    "\n",
    "            tau = model.get_tau()\n",
    "            loss_c = clip_loss(z_pred, z_true, tau=tau)\n",
    "            loss_a = smiles_align_loss_cosine(z_pred, z_true)\n",
    "            loss_smiles = loss_c + alpha_align * loss_a\n",
    "\n",
    "            loss = (lambda_targets * loss_targets + lambda_smiles * loss_smiles) / float(accum_steps)\n",
    "\n",
    "            if not torch.isfinite(loss).all():\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                continue\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            do_update = (step % accum_steps) == 0\n",
    "            if do_update:\n",
    "                if grad_clip is not None and grad_clip > 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                scheduler.step()\n",
    "                update_count += 1\n",
    "\n",
    "        # logging metrics (cheap)\n",
    "        with torch.no_grad():\n",
    "            m = smiles_retrieval_hitk(z_pred, drug_id, smiles_bank_t, k_list=(5,))\n",
    "            run_hit5 += float(m[\"Hit@5\"]) * bs\n",
    "            run_truecos += float(m[\"TrueCos\"]) * bs\n",
    "\n",
    "        run_total += float((lambda_targets * loss_targets + lambda_smiles * loss_smiles).item()) * bs\n",
    "        run_tgt   += float(loss_targets.item()) * bs\n",
    "        run_clip  += float(loss_c.item()) * bs\n",
    "        run_align += float(loss_a.item()) * bs\n",
    "        run_rank_last = float(loss_rank_t.item())\n",
    "\n",
    "        if step % log_every == 0:\n",
    "            lr_now = optimizer.param_groups[0][\"lr\"]\n",
    "            pbar.set_postfix({\n",
    "                \"lr\": f\"{lr_now:.2e}\",\n",
    "                \"tau\": f\"{float(model.get_tau().item()):.3f}\",\n",
    "                \"tot\": f\"{run_total/max(n,1):.4f}\",\n",
    "                \"tgt\": f\"{run_tgt/max(n,1):.4f}\",\n",
    "                \"clip\": f\"{run_clip/max(n,1):.4f}\",\n",
    "                \"align\": f\"{run_align/max(n,1):.4f}\",\n",
    "                \"rank(last)\": f\"{run_rank_last:.4f}\",\n",
    "                \"Hit@5\": f\"{run_hit5/max(n,1):.3f}\",\n",
    "                \"TrueCos\": f\"{run_truecos/max(n,1):.3f}\",\n",
    "                \"upd\": f\"{update_count}/{updates_per_epoch}\",\n",
    "            })\n",
    "\n",
    "    return {\n",
    "        \"train_total\": run_total / max(n,1),\n",
    "        \"train_tgt\": run_tgt / max(n,1),\n",
    "        \"train_clip\": run_clip / max(n,1),\n",
    "        \"train_align\": run_align / max(n,1),\n",
    "        \"train_hit5\": run_hit5 / max(n,1),\n",
    "        \"train_truecos\": run_truecos / max(n,1),\n",
    "        \"rank_last\": run_rank_last,\n",
    "        \"tau\": float(model.get_tau().item()),\n",
    "        \"lr\": optimizer.param_groups[0][\"lr\"],\n",
    "        \"updates\": update_count,\n",
    "    }\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 19) TRAIN\n",
    "# =========================================================\n",
    "print(\">>> TRAIN START: FP(TARGET-ONLY) + ORGAN + SMILES CLIP | Variant A(log1p->delta->clip) | NO pos_emb\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    logs = train_one_epoch_fixed_steps(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        device=device,\n",
    "        steps_per_epoch=STEPS_PER_EPOCH,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        scaler=scaler,\n",
    "        target_sub_ids=target_sub_ids,\n",
    "        pos_weight=pos_weight,\n",
    "        smiles_bank_t=smiles_bank_t,\n",
    "        log_every=50,\n",
    "        grad_clip=MAX_GRAD_NORM,\n",
    "        accum_steps=max(1, int(ACCUM_STEPS)),\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"\\n[Epoch {epoch}/{EPOCHS}] \"\n",
    "        f\"lr={logs['lr']:.2e} | tau={logs['tau']:.3f} | \"\n",
    "        f\"train_total={logs['train_total']:.4f} | \"\n",
    "        f\"train_tgt={logs['train_tgt']:.4f} | \"\n",
    "        f\"train_clip={logs['train_clip']:.4f} | \"\n",
    "        f\"train_align={logs['train_align']:.4f} | \"\n",
    "        f\"rank_last={logs['rank_last']:.4f} | \"\n",
    "        f\"Hit@5={logs['train_hit5']:.3f} | TrueCos={logs['train_truecos']:.3f}\"\n",
    "    )\n",
    "\n",
    "    valid = evaluate_fp(\n",
    "        model=model,\n",
    "        loader=val_loader,\n",
    "        device=device,\n",
    "        target_sub_ids=target_sub_ids,\n",
    "        smiles_bank_t=smiles_bank_t,\n",
    "        k_list=(5,10),\n",
    "        hitk=(1,5,10),\n",
    "    )\n",
    "    print(\"✅ VALID:\", valid)\n",
    "\n",
    "print(\">>> DONE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb7543d",
   "metadata": {},
   "source": [
    "## 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3182e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved FULL checkpoint (model+opt+sched+scaler+RNG+LUT): /data/aiffel/babayakga/checkpoints/f_p_final/fp_smalltargets.pt\n",
      "   - LUT saved: True\n",
      "   - optimizer saved: True\n",
      "   - scheduler saved: True\n",
      "   - scaler saved: True\n",
      "   - cuda rng saved: True\n"
     ]
    }
   ],
   "source": [
    "CKPT_DIR  = \"/data/aiffel/babayakga/checkpoints/f_p_final\"        \n",
    "CKPT_NAME = \"fp_smalltargets.pt\" \n",
    "ckpt_path = os.path.join(CKPT_DIR, CKPT_NAME)\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# 1) RNG states \n",
    "# =========================\n",
    "def get_rng_state_bundle():\n",
    "    out = {}\n",
    "    # python random\n",
    "    try:\n",
    "        out[\"python_random_state\"] = random.getstate()\n",
    "    except Exception as e:\n",
    "        out[\"python_random_state\"] = None\n",
    "        out[\"python_random_state_err\"] = repr(e)\n",
    "\n",
    "    # numpy\n",
    "    try:\n",
    "        out[\"numpy_random_state\"] = np.random.get_state()\n",
    "    except Exception as e:\n",
    "        out[\"numpy_random_state\"] = None\n",
    "        out[\"numpy_random_state_err\"] = repr(e)\n",
    "\n",
    "    # torch cpu\n",
    "    try:\n",
    "        out[\"torch_rng_state\"] = torch.get_rng_state()\n",
    "    except Exception as e:\n",
    "        out[\"torch_rng_state\"] = None\n",
    "        out[\"torch_rng_state_err\"] = repr(e)\n",
    "\n",
    "    # torch cuda \n",
    "    try:\n",
    "        if torch.cuda.is_available():\n",
    "            out[\"torch_cuda_rng_state_all\"] = torch.cuda.get_rng_state_all()\n",
    "        else:\n",
    "            out[\"torch_cuda_rng_state_all\"] = None\n",
    "    except Exception as e:\n",
    "        out[\"torch_cuda_rng_state_all\"] = None\n",
    "        out[\"torch_cuda_rng_state_all_err\"] = repr(e)\n",
    "\n",
    "    return out\n",
    "\n",
    "rng_bundle = get_rng_state_bundle()\n",
    "\n",
    "# =========================\n",
    "# 2) EXTRA\n",
    "# =========================\n",
    "EXTRA = {\n",
    "    \"SPECIAL_TOKENS\": SPECIAL_TOKENS,\n",
    "    \"N_SPECIAL\": int(N_SPECIAL),\n",
    "    \"VOCAB_SIZE\": int(VOCAB_SIZE),\n",
    "    \"PAD_ID\": int(PAD_ID),\n",
    "    \"CLS_ID\": int(CLS_ID),\n",
    "    \"ORGAN_TOK_ID\": int(ORGAN_TOK_ID),\n",
    "\n",
    "    \"subset_token_ids\": list(map(int, subset_token_ids)),\n",
    "    \"target_token_ids\": list(map(int, target_token_ids)),\n",
    "\n",
    "    \"UNK_ORGAN_ID\": int(UNK_ORGAN_ID),\n",
    "    \"organ2id\": organ2id,\n",
    "    \"NUM_ORGANS\": int(NUM_ORGANS),\n",
    "\n",
    "    \"D_MODEL\": int(D_MODEL),\n",
    "    \"N_HEADS\": int(N_HEADS),\n",
    "    \"N_LAYERS\": int(N_LAYERS),\n",
    "    \"MAX_SEQ_LEN\": int(MAX_SEQ_LEN),\n",
    "    \"SMILES_DIM\": int(SMILES_DIM),\n",
    "\n",
    "    \"SEED\": int(SEED),\n",
    "    \"CONTROL_DRUG\": str(CONTROL_DRUG),\n",
    "    \"HVG_K\": int(HVG_K),\n",
    "\n",
    "    \"USE_LOG1P_EXPR\": bool(USE_LOG1P_EXPR),\n",
    "    \"USE_ASINH_DELTA\": bool(USE_ASINH_DELTA),\n",
    "    \"DELTA_CLIP_ABS\": float(DELTA_CLIP_ABS),\n",
    "    \"DROP_FIRST_GENE_TOKEN\": bool(DROP_FIRST_GENE_TOKEN),\n",
    "}\n",
    "\n",
    "if \"old_tid_to_vocab_lut\" in globals() and isinstance(old_tid_to_vocab_lut, np.ndarray):\n",
    "    lut_tensor = torch.from_numpy(old_tid_to_vocab_lut.astype(np.int64, copy=False)).cpu()\n",
    "elif \"old_tid_to_vocab_lut\" in globals() and torch.is_tensor(old_tid_to_vocab_lut):\n",
    "    lut_tensor = old_tid_to_vocab_lut.detach().to(dtype=torch.int64, device=\"cpu\")\n",
    "else:\n",
    "    lut_tensor = None\n",
    "\n",
    "# =========================\n",
    "# 3) PAYLOAD\n",
    "# =========================\n",
    "payload = {\n",
    "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"model_class\": model.__class__.__name__,\n",
    "    \"model_state\": model.state_dict(),\n",
    "\n",
    "    \"optimizer_state\": optimizer.state_dict() if \"optimizer\" in globals() and optimizer is not None else None,\n",
    "    \"scheduler_state\": scheduler.state_dict() if \"scheduler\" in globals() and scheduler is not None else None,\n",
    "    \"scaler_state\": scaler.state_dict() if \"scaler\" in globals() and scaler is not None else None,\n",
    "\n",
    "    \"metrics\": {\"valid\": valid} if \"valid\" in globals() else {},\n",
    "\n",
    "    \"extra\": EXTRA,\n",
    "\n",
    "    # ✅ LUT inside checkpoint\n",
    "    \"old_tid_to_vocab_lut\": lut_tensor,\n",
    "\n",
    "    # ✅ RNG states\n",
    "    \"rng_state\": rng_bundle,\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# 4) ATOMIC SAVE\n",
    "# =========================\n",
    "tmp_path = ckpt_path + \".tmp\"\n",
    "torch.save(payload, tmp_path)\n",
    "os.replace(tmp_path, ckpt_path)\n",
    "\n",
    "print(f\"✅ Saved FULL checkpoint (model+opt+sched+scaler+RNG+LUT): {ckpt_path}\")\n",
    "print(f\"   - LUT saved: {lut_tensor is not None}\")\n",
    "print(f\"   - optimizer saved: {payload['optimizer_state'] is not None}\")\n",
    "print(f\"   - scheduler saved: {payload['scheduler_state'] is not None}\")\n",
    "print(f\"   - scaler saved: {payload['scaler_state'] is not None}\")\n",
    "print(f\"   - cuda rng saved: {payload['rng_state'].get('torch_cuda_rng_state_all') is not None}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28248d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ VALID: {'Recall@5': 0.2924789224068324, 'Precision@5': 0.08897396000723044, 'mAP@5': 0.19173023119471813, 'NDCG@5': 0.22381376816503082, 'Coverage@5_avg_targets': 2.095833333333333, 'Coverage@5_median_targets': 1.14, 'Coverage@5_frac_targets_le_k': 0.9517708333333333, 'Coverage@5_avg_recall_ceiling': 0.9826236150662104, 'Recall@5_over_ceiling': 0.2976510211258507, 'Recall@10': 0.4075360565384229, 'Precision@10': 0.06435156346609196, 'mAP@10': 0.21013024507517306, 'NDCG@10': 0.2646626206972481, 'Coverage@10_avg_targets': 2.095833333333333, 'Coverage@10_median_targets': 1.14, 'Coverage@10_frac_targets_le_k': 0.9886458333333333, 'Coverage@10_avg_recall_ceiling': 0.9976251810789108, 'Recall@10_over_ceiling': 0.4085061847553619, 'Recall@20': 0.5443653134504954, 'Precision@20': 0.0442695319528381, 'mAP@20': 0.2221575258799324, 'NDCG@20': 0.30403822676472675, 'Coverage@20_avg_targets': 2.095833333333333, 'Coverage@20_median_targets': 1.14, 'Coverage@20_frac_targets_le_k': 1.0, 'Coverage@20_avg_recall_ceiling': 1.0, 'Recall@20_over_ceiling': 0.5443653134504954, 'SMILES_Hit@1': 0.12861979166666668, 'SMILES_Hit@5': 0.268203125, 'SMILES_Hit@10': 0.34294270833333335, 'SMILES_TrueCos': 0.4066869940360387, 'SMILES_MRR': 0.20479313254356385, 'SMILES_median_rank': 32.126666666666665, 'SMILES_mean_rank': 72.740703125, 'SMILES_CLIP': 3.7804848623275755, 'tau': 0.024286000058054924, 'n_samples': 38400.0}\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Tuple, Iterable, List\n",
    "# =========================================================\n",
    "# Ranking metrics for targets: mAP@K, NDCG@K, Coverage ceiling\n",
    "# =========================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def average_precision_at_k(scores_1d: torch.Tensor, y_true_1d: torch.Tensor, k: int) -> float:\n",
    "    \"\"\"\n",
    "    AP@K for ONE sample (binary relevance).\n",
    "    scores_1d: (M,)\n",
    "    y_true_1d: (M,) in {0,1}\n",
    "    \"\"\"\n",
    "    M = scores_1d.numel()\n",
    "    kk = min(int(k), int(M))\n",
    "    if kk <= 0:\n",
    "        return 0.0\n",
    "\n",
    "    pos_total = float(y_true_1d.sum().item())\n",
    "    if pos_total <= 0:\n",
    "        return 0.0\n",
    "\n",
    "    topk = torch.topk(scores_1d, k=kk, dim=0).indices\n",
    "    rel = y_true_1d[topk].float()  # (kk,)\n",
    "\n",
    "    # precision@i only when rel[i]=1\n",
    "    cumsum_rel = torch.cumsum(rel, dim=0)\n",
    "    ranks = torch.arange(1, kk + 1, device=scores_1d.device, dtype=torch.float32)\n",
    "    precision_i = cumsum_rel / ranks\n",
    "\n",
    "    ap = (precision_i * rel).sum() / max(1.0, min(pos_total, float(kk)))\n",
    "    return float(ap.item())\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def mean_average_precision_at_k(scores: torch.Tensor, y_true: torch.Tensor, k: int) -> float:\n",
    "    \"\"\"\n",
    "    mAP@K over batch (binary relevance).\n",
    "    scores: (B, M)\n",
    "    y_true: (B, M) in {0,1}\n",
    "    \"\"\"\n",
    "    B = scores.size(0)\n",
    "    aps = []\n",
    "    for i in range(B):\n",
    "        aps.append(average_precision_at_k(scores[i], y_true[i], k))\n",
    "    return float(sum(aps) / max(1, len(aps)))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def ndcg_at_k(scores_1d: torch.Tensor, y_true_1d: torch.Tensor, k: int) -> float:\n",
    "    \"\"\"\n",
    "    NDCG@K for ONE sample (binary relevance).\n",
    "    DCG = sum_{i=1..K} rel_i / log2(i+1)\n",
    "    IDCG computed from sorted relevances (all ones first).\n",
    "    \"\"\"\n",
    "    M = scores_1d.numel()\n",
    "    kk = min(int(k), int(M))\n",
    "    if kk <= 0:\n",
    "        return 0.0\n",
    "\n",
    "    pos_total = int(y_true_1d.sum().item())\n",
    "    if pos_total <= 0:\n",
    "        return 0.0\n",
    "\n",
    "    topk = torch.topk(scores_1d, k=kk, dim=0).indices\n",
    "    rel = y_true_1d[topk].float()  # (kk,)\n",
    "\n",
    "    denom = torch.log2(torch.arange(2, kk + 2, device=scores_1d.device, dtype=torch.float32))\n",
    "    dcg = (rel / denom).sum()\n",
    "\n",
    "    ideal_k = min(pos_total, kk)\n",
    "    ideal_rel = torch.ones((ideal_k,), device=scores_1d.device, dtype=torch.float32)\n",
    "    idcg = (ideal_rel / denom[:ideal_k]).sum()\n",
    "\n",
    "    return float((dcg / (idcg + 1e-12)).item())\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def mean_ndcg_at_k(scores: torch.Tensor, y_true: torch.Tensor, k: int) -> float:\n",
    "    B = scores.size(0)\n",
    "    vals = []\n",
    "    for i in range(B):\n",
    "        vals.append(ndcg_at_k(scores[i], y_true[i], k))\n",
    "    return float(sum(vals) / max(1, len(vals)))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def recall_precision_at_k(scores: torch.Tensor, y_true: torch.Tensor, k: int) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Same spirit as your compute_recall_precision_at_k, but vectorized-ish and robust.\n",
    "    \"\"\"\n",
    "    B, M = scores.shape\n",
    "    kk = min(int(k), int(M))\n",
    "    if kk <= 0:\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    topk = torch.topk(scores, k=kk, dim=1).indices  # (B, kk)\n",
    "    rel = torch.gather(y_true, 1, topk).float()     # (B, kk)\n",
    "    pos_total = y_true.sum(dim=1).float()           # (B,)\n",
    "\n",
    "    mask = pos_total > 0\n",
    "    if not mask.any():\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    num_pos_in_topk = rel.sum(dim=1)  # (B,)\n",
    "    recall = (num_pos_in_topk[mask] / (pos_total[mask] + 1e-12)).mean()\n",
    "    precision = (num_pos_in_topk[mask] / float(kk)).mean()\n",
    "    return float(recall.item()), float(precision.item())\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def coverage_ceiling_recall_at_k(y_true: torch.Tensor, k: int) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Coverage vs #targets:\n",
    "    - avg #targets\n",
    "    - fraction of samples with <=k targets\n",
    "    - avg ceiling recall@k = min(k, #targets) / #targets\n",
    "    - median #targets\n",
    "    \"\"\"\n",
    "    k = int(k)\n",
    "    num_t = y_true.sum(dim=1).float()  # (B,)\n",
    "    mask = num_t > 0\n",
    "    if not mask.any():\n",
    "        return {\n",
    "            \"avg_targets\": 0.0,\n",
    "            \"median_targets\": 0.0,\n",
    "            \"frac_targets_le_k\": 0.0,\n",
    "            \"avg_recall_ceiling\": 0.0,\n",
    "        }\n",
    "\n",
    "    nt = num_t[mask]\n",
    "    ceiling = torch.minimum(nt, torch.tensor(float(k), device=y_true.device)) / (nt + 1e-12)\n",
    "    frac_le = (nt <= float(k)).float().mean()\n",
    "\n",
    "    # median (torch median)\n",
    "    median = nt.median()\n",
    "\n",
    "    return {\n",
    "        \"avg_targets\": float(nt.mean().item()),\n",
    "        \"median_targets\": float(median.item()),\n",
    "        \"frac_targets_le_k\": float(frac_le.item()),\n",
    "        \"avg_recall_ceiling\": float(ceiling.mean().item()),\n",
    "    }\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Retrieval metrics for SMILES: MRR, median rank, plus Hit@K, TrueCos\n",
    "# =========================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def smiles_retrieval_metrics(\n",
    "    z_pred: torch.Tensor,\n",
    "    drug_id: torch.Tensor,\n",
    "    smiles_bank_t: torch.Tensor,\n",
    "    k_list: Iterable[int] = (1, 5, 10),\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    z_pred: (B, D) predicted SMILES vector\n",
    "    drug_id: (B,) true drug index in bank\n",
    "    smiles_bank_t: (N, D)\n",
    "    Returns: Hit@K, TrueCos, MRR, median_rank, mean_rank\n",
    "    \"\"\"\n",
    "    z = F.normalize(z_pred.float(), dim=1)\n",
    "    b = F.normalize(smiles_bank_t.float(), dim=1)\n",
    "\n",
    "    logits = z @ b.T  # (B, N)\n",
    "    B, N = logits.shape\n",
    "\n",
    "    # ranks: higher logits => better\n",
    "    # rank = 1 + number of items with score > true_score (ties -> worst-ish; acceptable)\n",
    "    true_scores = logits.gather(1, drug_id.view(-1, 1))  # (B,1)\n",
    "    better = (logits > true_scores).sum(dim=1)           # (B,)\n",
    "    rank = better + 1                                   # (B,) 1..N\n",
    "\n",
    "    out = {}\n",
    "\n",
    "    # Hit@K\n",
    "    for k in k_list:\n",
    "        k = min(int(k), N)\n",
    "        topk = torch.topk(logits, k=k, dim=1).indices\n",
    "        hit = (topk == drug_id.view(-1, 1)).any(dim=1).float().mean()\n",
    "        out[f\"Hit@{k}\"] = float(hit.item())\n",
    "\n",
    "    # TrueCos\n",
    "    true_vec = b[drug_id]\n",
    "    out[\"TrueCos\"] = float((z * true_vec).sum(dim=1).mean().item())\n",
    "\n",
    "    # MRR / ranks\n",
    "    out[\"MRR\"] = float((1.0 / rank.float()).mean().item())\n",
    "    out[\"median_rank\"] = float(rank.float().median().item())\n",
    "    out[\"mean_rank\"] = float(rank.float().mean().item())\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Drop-in replacement: evaluate_fp + new metrics\n",
    "# =========================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_fp_with_ranking_and_retrieval(\n",
    "    model,\n",
    "    loader,\n",
    "    device,\n",
    "    target_sub_ids,\n",
    "    smiles_bank_t,\n",
    "    k_list_targets: Iterable[int] = (5, 10, 20),\n",
    "    k_list_smiles: Iterable[int] = (1, 5, 10),\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Adds:\n",
    "      - Targets: mAP@K, NDCG@K, Recall@K, Precision@K + Coverage ceiling stats\n",
    "      - SMILES: Hit@K, TrueCos, MRR, median rank, mean rank, CLIP loss, tau\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    gene_emb = model.gene_emb_subset()[target_sub_ids].to(device)  # (M_TGT, d)\n",
    "    g_norm = F.normalize(gene_emb, dim=1)\n",
    "\n",
    "    # accumulators\n",
    "    out_sum = defaultdict(float)\n",
    "    n_batches = 0\n",
    "    n_samples = 0\n",
    "\n",
    "    clip_sum = 0.0\n",
    "    tau_sum = 0.0\n",
    "\n",
    "    # coverage stats accum (per K)\n",
    "    cov_sums = {k: defaultdict(float) for k in k_list_targets}\n",
    "    cov_counts = {k: 0 for k in k_list_targets}\n",
    "\n",
    "    for batch in loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "        values    = batch[\"values\"].to(device, non_blocking=True)\n",
    "        attn      = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "        y_targets = batch[\"y_targets\"].to(device, non_blocking=True)  # (B, M_TGT)\n",
    "        z_true    = batch[\"smiles_emb\"].to(device, non_blocking=True)\n",
    "        drug_id   = batch[\"drug_id\"].to(device, non_blocking=True)\n",
    "        organ_id  = batch[\"organ_id\"].to(device, non_blocking=True)\n",
    "\n",
    "        v_pred, z_pred = model(input_ids, values, attn, organ_id=organ_id, return_smiles=True)\n",
    "        v_norm = F.normalize(v_pred, dim=1)\n",
    "        scores = v_norm @ g_norm.T  # (B, M_TGT)\n",
    "\n",
    "        B = scores.size(0)\n",
    "        n_batches += 1\n",
    "        n_samples += B\n",
    "\n",
    "        # ---- Targets metrics ----\n",
    "        for k in k_list_targets:\n",
    "            r, p = recall_precision_at_k(scores, y_targets, k=k)\n",
    "            ap = mean_average_precision_at_k(scores, y_targets, k=k)\n",
    "            nd = mean_ndcg_at_k(scores, y_targets, k=k)\n",
    "\n",
    "            out_sum[f\"Recall@{k}\"] += r\n",
    "            out_sum[f\"Precision@{k}\"] += p\n",
    "            out_sum[f\"mAP@{k}\"] += ap\n",
    "            out_sum[f\"NDCG@{k}\"] += nd\n",
    "\n",
    "            cov = coverage_ceiling_recall_at_k(y_targets, k=k)\n",
    "            for kk, vv in cov.items():\n",
    "                cov_sums[k][kk] += float(vv)\n",
    "            cov_counts[k] += 1\n",
    "\n",
    "        # ---- SMILES retrieval metrics ----\n",
    "        m = smiles_retrieval_metrics(z_pred, drug_id, smiles_bank_t, k_list=k_list_smiles)\n",
    "        for key, val in m.items():\n",
    "            out_sum[f\"SMILES_{key}\"] += float(val) * B  # weight by batch size\n",
    "\n",
    "        # ---- CLIP loss / tau ----\n",
    "        tau = model.get_tau()\n",
    "        clip_sum += float(clip_loss(z_pred, z_true, tau=tau).item()) * B\n",
    "        tau_sum  += float(tau.item()) * B\n",
    "\n",
    "    out = {}\n",
    "\n",
    "    # Average batch-averaged target metrics\n",
    "    for k in k_list_targets:\n",
    "        out[f\"Recall@{k}\"] = out_sum[f\"Recall@{k}\"] / max(1, n_batches)\n",
    "        out[f\"Precision@{k}\"] = out_sum[f\"Precision@{k}\"] / max(1, n_batches)\n",
    "        out[f\"mAP@{k}\"] = out_sum[f\"mAP@{k}\"] / max(1, n_batches)\n",
    "        out[f\"NDCG@{k}\"] = out_sum[f\"NDCG@{k}\"] / max(1, n_batches)\n",
    "\n",
    "        # Coverage ceiling stats (averaged over batches)\n",
    "        cc = cov_counts[k]\n",
    "        if cc > 0:\n",
    "            out[f\"Coverage@{k}_avg_targets\"] = cov_sums[k][\"avg_targets\"] / cc\n",
    "            out[f\"Coverage@{k}_median_targets\"] = cov_sums[k][\"median_targets\"] / cc\n",
    "            out[f\"Coverage@{k}_frac_targets_le_k\"] = cov_sums[k][\"frac_targets_le_k\"] / cc\n",
    "            out[f\"Coverage@{k}_avg_recall_ceiling\"] = cov_sums[k][\"avg_recall_ceiling\"] / cc\n",
    "            # optional: \"normalized recall\" = Recall@K / ceiling (if you want)\n",
    "            ceil = out[f\"Coverage@{k}_avg_recall_ceiling\"]\n",
    "            out[f\"Recall@{k}_over_ceiling\"] = out[f\"Recall@{k}\"] / max(ceil, 1e-9)\n",
    "\n",
    "    # SMILES metrics averaged over samples (we weighted by B already)\n",
    "    for key in [\"Hit@1\", \"Hit@5\", \"Hit@10\", \"TrueCos\", \"MRR\", \"median_rank\", \"mean_rank\"]:\n",
    "        sk = f\"SMILES_{key}\"\n",
    "        if sk in out_sum:\n",
    "            out[sk] = out_sum[sk] / max(1, n_samples)\n",
    "\n",
    "    out[\"SMILES_CLIP\"] = clip_sum / max(1, n_samples)\n",
    "    out[\"tau\"] = tau_sum / max(1, n_samples)\n",
    "    out[\"n_samples\"] = float(n_samples)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "valid = evaluate_fp_with_ranking_and_retrieval(\n",
    "    model=model,\n",
    "    loader=val_loader,\n",
    "    device=device,\n",
    "    target_sub_ids=target_sub_ids,\n",
    "    smiles_bank_t=smiles_bank_t,\n",
    "    k_list_targets=(5, 10, 20),\n",
    "    k_list_smiles=(1, 5, 10),\n",
    ")\n",
    "print(\"✅ VALID:\", valid)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "babayakga",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
